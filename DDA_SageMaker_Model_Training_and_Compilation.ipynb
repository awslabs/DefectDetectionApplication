{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0fa16f",
   "metadata": {},
   "source": [
    "# SageMaker Training for DDA (Defect Detection Application)\n",
    "\n",
    "This notebook demonstrates how to train and compile computer vision models for defect detection using Amazon SageMaker. The workflow includes:\n",
    "1. Training classification and segmentation models\n",
    "2. Compiling models for different target devices (x86_64, ARM64, Jetson Xavier)\n",
    "3. Preparing models for deployment with DDA edge application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2fa15",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **Environment**: Open this notebook in Amazon SageMaker Notebook Instance or SageMaker Studio\n",
    "2. **SageMaker Experience**: Basic familiarity with Amazon SageMaker\n",
    "3. **Marketplace Access**: Either:\n",
    "   - IAM permissions for AWS Marketplace operations:\n",
    "     - `aws-marketplace:ViewSubscriptions`\n",
    "     - `aws-marketplace:Unsubscribe`\n",
    "     - `aws-marketplace:Subscribe`\n",
    "   - OR existing subscription to [Computer Vision Defect Detection Model](https://aws.amazon.com/marketplace/pp/prodview-j72hhmlt6avp6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53e78bd",
   "metadata": {},
   "source": [
    "## Step 1: Subscribe to Algorithm\n",
    "\n",
    "To use the defect detection algorithm:\n",
    "\n",
    "1. Visit: [Computer Vision Defect Detection Model](https://aws.amazon.com/marketplace/pp/prodview-j72hhmlt6avp6)\n",
    "2. Click **Continue to subscribe**\n",
    "3. Review and **Accept Offer** (EULA, pricing, support terms)\n",
    "4. Click **Continue to configuration** and select your region\n",
    "5. Copy the **Product ARN** and paste it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your subscribed algorithm ARN from AWS Marketplace\n",
    "algorithm_name = \"<Customer to specify the algorithm name after subscription>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a681a",
   "metadata": {},
   "source": [
    "## Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fd630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450d61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session and get default configurations\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Project identifier for S3 output paths\n",
    "project = \"LFV-public-test\"\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"S3 Bucket: {bucket}\")\n",
    "print(f\"Project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca0d53",
   "metadata": {},
   "source": [
    "## Step 3: Setup Sample Cookie Dataset (Optional)\n",
    "\n",
    "**For Testing Only:** Download and prepare sample cookie dataset from Lookout for Vision.\n",
    "\n",
    "**Skip this entire section if you have:**\n",
    "- Your own labeled training images\n",
    "- Custom manifest files pointing to your S3 data\n",
    "- Data from SageMaker Ground Truth labeling jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb531a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup S3 paths and create project structure\n",
    "s3_client = boto3.client('s3')\n",
    "s3_uri = f\"s3://{bucket}/{project}/\"\n",
    "\n",
    "# Create S3 folder structure\n",
    "folders = ['', 'output/', 'compilation_output/']\n",
    "for folder in folders:\n",
    "    s3_client.put_object(Bucket=bucket, Key=f\"{project}/{folder}\")\n",
    "\n",
    "# Define paths for later use\n",
    "output_path = f's3://{bucket}/{project}/output'\n",
    "compilation_output_path = f's3://{bucket}/{project}/compilation_output'\n",
    "\n",
    "print(f\"‚úÖ S3 structure created: {s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download cookie dataset from GitHub\n",
    "!git clone --depth 1 https://github.com/aws-samples/amazon-lookout-for-vision.git\n",
    "!cp -r amazon-lookout-for-vision/computer-vision-defect-detection/cookie-dataset ./\n",
    "!rm -rf amazon-lookout-for-vision\n",
    "\n",
    "print(f\"‚úÖ Cookie dataset downloaded ({len(os.listdir('cookie-dataset/dataset-files/training-images'))} images)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and upload dataset to S3\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Copy dataset files locally\n",
    "if os.path.exists('dataset-files'):\n",
    "    shutil.rmtree('dataset-files')\n",
    "shutil.copytree('cookie-dataset/dataset-files', 'dataset-files')\n",
    "\n",
    "# Copy mask file\n",
    "if os.path.exists('cookie-dataset/dummy_anomaly_mask.png'):\n",
    "    shutil.copy2('cookie-dataset/dummy_anomaly_mask.png', 'dataset-files/mask-images/')\n",
    "\n",
    "# Upload dataset to S3 using getting_started.py script\n",
    "!python cookie-dataset/getting_started.py {s3_uri}\n",
    "\n",
    "# Set manifest URIs\n",
    "training_manifest_s3_uri = f\"{s3_uri}manifests/train.manifest\"\n",
    "\n",
    "print(f\"‚úÖ Dataset uploaded to S3: {training_manifest_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seg_manifest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process segmentation manifest\n",
    "import json\n",
    "\n",
    "# Download segmentation manifest from GitHub\n",
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-lookout-for-vision/d4002d64b1ba395d332b994a0c268342ac62b1ed/computer-vision-defect-detection/train_segmentation.manifest\n",
    "\n",
    "# Update manifest with current S3 bucket\n",
    "def update_manifest_paths(manifest_file, old_prefix, new_prefix):\n",
    "    updated_lines = []\n",
    "    with open(manifest_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            for key in ['source-ref', 'anomaly-mask-ref']:\n",
    "                if key in data and data[key].startswith(old_prefix):\n",
    "                    data[key] = data[key].replace(old_prefix, new_prefix)\n",
    "            updated_lines.append(json.dumps(data))\n",
    "    return updated_lines\n",
    "\n",
    "# Process manifest\n",
    "old_prefix = 's3://lookoutvision-us-east-1-0e205be246/getting-started/'\n",
    "segmentation_lines = update_manifest_paths('train_segmentation.manifest', old_prefix, s3_uri)\n",
    "\n",
    "# Save updated manifest\n",
    "seg_manifest_path = 'dataset-files/manifests/train_segmentation.manifest'\n",
    "with open(seg_manifest_path, 'w') as f:\n",
    "    f.write('\\n'.join(segmentation_lines))\n",
    "\n",
    "# Upload to S3\n",
    "s3_key = f\"{project}/manifests/train_segmentation.manifest\"\n",
    "s3_client.upload_file(seg_manifest_path, bucket, s3_key)\n",
    "segmentation_manifest_s3_uri = f\"s3://{bucket}/{s3_key}\"\n",
    "\n",
    "# Cleanup\n",
    "os.remove('train_segmentation.manifest')\n",
    "\n",
    "print(f\"‚úÖ Segmentation manifest created: {segmentation_manifest_s3_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of created resources\n",
    "print(\"üéâ Sample Dataset Setup Complete:\")\n",
    "print(f\"üìÅ Training images: {s3_uri}training-images/\")\n",
    "print(f\"üé≠ Mask images: {s3_uri}mask-images/\")\n",
    "print(f\"üìã Classification manifest: {training_manifest_s3_uri}\")\n",
    "print(f\"üîç Segmentation manifest: {segmentation_manifest_s3_uri}\")\n",
    "print(f\"üìä Output path: {output_path}\")\n",
    "print(f\"‚öôÔ∏è Compilation path: {compilation_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d5faf",
   "metadata": {},
   "source": [
    "## Step 4: Get SageMaker Execution Role\n",
    "\n",
    "Get the current SageMaker execution role for training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c284ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current execution role\n",
    "sm_role_arn = sagemaker.get_execution_role()\n",
    "print(f\"Current SageMaker execution role ARN: {sm_role_arn}\")\n",
    "\n",
    "# Now you can use sm_role_arn in your SageMaker operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad86092",
   "metadata": {},
   "source": [
    "## Step 5: Train Classification Model\n",
    "\n",
    "Start training job for binary classification (normal vs anomaly detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker client and create unique job name\n",
    "sagemaker_client = boto3.Session(region_name=region).client(\"sagemaker\")\n",
    "classification_training_job_name = 'LFV-classification-' + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "print(f\"Classification training job: {classification_training_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7019056c",
   "metadata": {},
   "source": [
    "**Model Type Options:**\n",
    "- `classification`: Standard classification model\n",
    "- `classification-robust`: Enhanced model with improved robustness\n",
    "\n",
    "**Data Attributes:**\n",
    "- `source-ref`: Image file location\n",
    "- `anomaly-label-metadata`: Label metadata\n",
    "- `anomaly-label`: Binary classification label (normal/anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9990c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification training job\n",
    "response = sagemaker_client.create_training_job(\n",
    "    TrainingJobName=classification_training_job_name,\n",
    "    \n",
    "    # Model configuration\n",
    "    HyperParameters={\n",
    "        'ModelType': 'classification',  # Use 'classification-robust' for enhanced model\n",
    "        'TestInputDataAttributeNames': 'source-ref,anomaly-label-metadata,anomaly-label',\n",
    "        'TrainingInputDataAttributeNames': 'source-ref,anomaly-label-metadata,anomaly-label'\n",
    "    },\n",
    "    \n",
    "    # Algorithm specification\n",
    "    AlgorithmSpecification={\n",
    "        'AlgorithmName': algorithm_name,\n",
    "        'TrainingInputMode': 'File',\n",
    "        'EnableSageMakerMetricsTimeSeries': False\n",
    "    },\n",
    "    \n",
    "    # IAM role for training\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Training data configuration\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'training',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'AugmentedManifestFile',\n",
    "                    'S3Uri': 's3://lookoutvision-us-east-1-0e205be246/getting-started/manifests/train_class.manifest',\n",
    "                    'S3DataDistributionType': 'ShardedByS3Key',\n",
    "                    'AttributeNames': [\n",
    "                        'source-ref',\n",
    "                        'anomaly-label-metadata',\n",
    "                        'anomaly-label'\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'RecordIO',\n",
    "            'InputMode': 'Pipe'\n",
    "        },\n",
    "    ],\n",
    "    \n",
    "    # Output configuration\n",
    "    OutputDataConfig={'S3OutputPath': f's3://{bucket}/{project}/output'},\n",
    "    \n",
    "    # Compute resources\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.g4dn.2xlarge',  # GPU instance for faster training\n",
    "        'InstanceCount': 1,\n",
    "        'VolumeSizeInGB': 20\n",
    "    },\n",
    "    \n",
    "    # Training time limit (2 hours)\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 7200\n",
    "    }\n",
    "\n",
    "    # Enable network isolation for security\n",
    "    EnableNetworkIsolation=True \n",
    ")\n",
    "\n",
    "print(\"Classification training job started successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor classification training progress\n",
    "print(\"Monitoring classification training progress...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    training_response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=classification_training_job_name\n",
    "    )\n",
    "    \n",
    "    status = training_response['TrainingJobStatus']\n",
    "    \n",
    "    if status == 'InProgress':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'Completed':\n",
    "        print(\"\\nClassification training completed successfully!\")\n",
    "        break\n",
    "    elif status == 'Failed':\n",
    "        print(\"\\nClassification training failed!\")\n",
    "        print(f\"Failure reason: {training_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)  # Check every minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38efdd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f8031e",
   "metadata": {},
   "source": [
    "## Step 6: Train Segmentation Model\n",
    "\n",
    "Start training job for pixel-level segmentation (identifies exact defect locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de783648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique segmentation training job name\n",
    "segmentation_training_job_name = 'LFV-segmentation-' + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "print(f\"Segmentation training job: {segmentation_training_job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d291b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segmentation training job\n",
    "response = sagemaker_client.create_training_job(\n",
    "    TrainingJobName=segmentation_training_job_name,\n",
    "    \n",
    "    # Model configuration for segmentation\n",
    "    HyperParameters={\n",
    "        'ModelType': 'segmentation',  # Use 'segmentation-robust' for enhanced model\n",
    "        'TestInputDataAttributeNames': 'source-ref,anomaly-label-metadata,anomaly-label,anomaly-mask-ref-metadata,anomaly-mask-ref',\n",
    "        'TrainingInputDataAttributeNames': 'source-ref,anomaly-label-metadata,anomaly-label,anomaly-mask-ref-metadata,anomaly-mask-ref'\n",
    "        # Optional: Add 'classification_logic': 'seg_head' to use segmentation head only\n",
    "    },\n",
    "    \n",
    "    # Algorithm specification\n",
    "    AlgorithmSpecification={\n",
    "        'AlgorithmName': algorithm_name,\n",
    "        'TrainingInputMode': 'File',\n",
    "        'EnableSageMakerMetricsTimeSeries': False\n",
    "    },\n",
    "    \n",
    "    # IAM role for training\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Training data with mask annotations\n",
    "    InputDataConfig=[\n",
    "        {\n",
    "            'ChannelName': 'training',\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'AugmentedManifestFile',\n",
    "                    'S3Uri': 's3://lookoutvision-us-east-1-0e205be246/getting-started/manifests/train_segmentation.manifest',\n",
    "                    'S3DataDistributionType': 'ShardedByS3Key',\n",
    "                    'AttributeNames': [\n",
    "                        'source-ref',\n",
    "                        'anomaly-label-metadata',\n",
    "                        'anomaly-label',\n",
    "                        'anomaly-mask-ref-metadata',  # Segmentation mask metadata\n",
    "                        'anomaly-mask-ref'            # Segmentation mask file\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "            'CompressionType': 'None',\n",
    "            'RecordWrapperType': 'RecordIO',\n",
    "            'InputMode': 'Pipe'\n",
    "        },\n",
    "    ],\n",
    "    \n",
    "    # Output configuration\n",
    "    OutputDataConfig={'S3OutputPath': f's3://{bucket}/{project}/output'},\n",
    "    \n",
    "    # Compute resources\n",
    "    ResourceConfig={\n",
    "        'InstanceType': 'ml.g4dn.2xlarge',\n",
    "        'InstanceCount': 1,\n",
    "        'VolumeSizeInGB': 20\n",
    "    },\n",
    "    \n",
    "    # Training time limit\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 7200\n",
    "    }\n",
    "    # Enable network isolation for security\n",
    "    EnableNetworkIsolation=True \n",
    ")\n",
    "\n",
    "print(\"Segmentation training job started successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor segmentation training progress\n",
    "print(\"Monitoring segmentation training progress...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    training_response = sagemaker_client.describe_training_job(\n",
    "        TrainingJobName=segmentation_training_job_name\n",
    "    )\n",
    "    \n",
    "    status = training_response['TrainingJobStatus']\n",
    "    \n",
    "    if status == 'InProgress':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'Completed':\n",
    "        print(\"\\nSegmentation training completed successfully!\")\n",
    "        break\n",
    "    elif status == 'Failed':\n",
    "        print(\"\\nSegmentation training failed!\")\n",
    "        print(f\"Failure reason: {training_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348560fc",
   "metadata": {},
   "source": [
    "**Segmentation Model Options:**\n",
    "\n",
    "1. **Segmentation Head Only:**\n",
    "   ```python\n",
    "   HyperParameters={\n",
    "       'ModelType': 'segmentation',\n",
    "       'classification_logic': 'seg_head',\n",
    "       # ... other parameters\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Robust Segmentation Model:**\n",
    "   ```python\n",
    "   HyperParameters={\n",
    "       'ModelType': 'segmentation-robust',\n",
    "       # ... other parameters\n",
    "   }\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f7443",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13f24b",
   "metadata": {},
   "source": [
    "## Step 7: Model Compilation - Classification\n",
    "\n",
    "Compile the trained classification model for different target devices. SageMaker Neo optimizes models for specific hardware platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b1951b",
   "metadata": {},
   "source": [
    "### Prepare Model for Compilation\n",
    "\n",
    "SageMaker compilation requires a single PyTorch model file. We need to:\n",
    "1. Download the trained model artifact\n",
    "2. Extract and repackage the `mochi.pt` file\n",
    "3. Upload to S3 for compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1009b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification model artifact location\n",
    "res_class = sagemaker_client.describe_training_job(TrainingJobName=classification_training_job_name)\n",
    "output_model_path = res_class['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Classification model artifact: {output_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse S3 URI to extract bucket and key\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed_url = urlparse(output_model_path)\n",
    "output_bucket = parsed_url.netloc\n",
    "output_key = parsed_url.path.lstrip('/')\n",
    "\n",
    "print(f\"S3 Bucket: {output_bucket}\")\n",
    "print(f\"S3 Key: {output_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download, extract, and repackage model for compilation\n",
    "import tarfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "path = \"./classification\"\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download model artifact from S3\n",
    "input_tar_gz = os.path.join(path, 'model.tar.gz')\n",
    "s3_client.download_file(output_bucket, output_key, input_tar_gz)\n",
    "print(f\"Downloaded model artifact to {input_tar_gz}\")\n",
    "\n",
    "# Extract the model archive\n",
    "extract_dir = os.path.join(path, 'extracted')\n",
    "Path(extract_dir).mkdir(parents=True, exist_ok=True)\n",
    "with tarfile.open(input_tar_gz, 'r:gz') as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "print(f\"Extracted contents to {extract_dir}\")\n",
    "\n",
    "# Find the mochi.pt model file\n",
    "model_file = os.path.join(extract_dir, 'mochi.pt')\n",
    "if not os.path.exists(model_file):\n",
    "    raise FileNotFoundError(\"mochi.pt file not found in extracted contents\")\n",
    "print(f\"Found model file: {model_file}\")\n",
    "\n",
    "# Extract input_shape from mochi.json\n",
    "mochi_json_path = os.path.join(extract_dir, 'mochi.json')\n",
    "if not os.path.exists(mochi_json_path):\n",
    "    raise FileNotFoundError(\"mochi.json file not found in extracted contents\")\n",
    "\n",
    "print(f\"Found mochi.json file: {mochi_json_path}\")\n",
    "with open(mochi_json_path, 'r') as f:\n",
    "    mochi_data = json.load(f)\n",
    "    input_shape = mochi_data['stages'][0]['input_shape']\n",
    "    print(f\"Extracted input_shape: {input_shape}\")\n",
    "    \n",
    "    # Extract height and width\n",
    "    height = input_shape[2]\n",
    "    width = input_shape[3]\n",
    "    \n",
    "    # Build tensor shape for DataInputConfig\n",
    "    tensor_shape = [1, 3, height, width]\n",
    "    classification_data_input_config = json.dumps({\"input_shape\": tensor_shape})\n",
    "    \n",
    "    print(f\"Height: {height}, Width: {width}\")\n",
    "    print(f\"Classification DataInputConfig: {classification_data_input_config}\")\n",
    "\n",
    "# Create new archive with just the model file\n",
    "output_tar_gz = os.path.join(path, 'classification.tar.gz')\n",
    "with tarfile.open(output_tar_gz, \"w:gz\") as tar:\n",
    "    tar.add(model_file, arcname=os.path.basename(model_file))\n",
    "print(f\"Created compilation-ready archive: {output_tar_gz}\")\n",
    "\n",
    "# Upload repackaged model to S3\n",
    "target_key = output_key.rsplit('/', 1)[0] + '/classification.tar.gz'\n",
    "s3_client.upload_file(output_tar_gz, output_bucket, target_key)\n",
    "print(f\"Uploaded to s3://{output_bucket}/{target_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4f4c1",
   "metadata": {},
   "source": [
    "### Compile for Jetson Xavier (JetPack 4)\n",
    "\n",
    "Target: ARM64 architecture with NVIDIA GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b4b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compilation job for Jetson Xavier\n",
    "compilation_job_name = \"class-xavier-gpu-\" + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "compressed_model_path = f\"s3://{output_bucket}/{target_key}\"\n",
    "\n",
    "print(f\"Compilation job: {compilation_job_name}\")\n",
    "print(f\"Model path: {compressed_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21225218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start compilation for Jetson Xavier\n",
    "create_response = sagemaker_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Input model configuration\n",
    "    InputConfig={\n",
    "        'S3Uri': compressed_model_path,\n",
    "        'DataInputConfig': classification_data_input_config,\n",
    "        'Framework': 'PYTORCH',\n",
    "        'FrameworkVersion': '1.8'\n",
    "    },\n",
    "    \n",
    "    # Output and target platform configuration\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket}/{project}/compilation_output',\n",
    "        'TargetPlatform': {\n",
    "            'Os': 'LINUX',\n",
    "            'Arch': 'ARM64',\n",
    "            'Accelerator': 'NVIDIA'  # GPU acceleration\n",
    "        },\n",
    "        # Jetson Xavier specific compiler options\n",
    "        'CompilerOptions': '{\"cuda-ver\": \"10.2\",\"gpu-code\": \"sm_72\",\"trt-ver\": \"8.2.1\"}'\n",
    "    },\n",
    "    \n",
    "    # Compilation time limit (1 hour)\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Jetson Xavier compilation job started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor Jetson Xavier compilation progress\n",
    "print(\"Monitoring Jetson Xavier compilation...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    compile_response = sagemaker_client.describe_compilation_job(\n",
    "        CompilationJobName=compilation_job_name\n",
    "    )\n",
    "    \n",
    "    status = compile_response['CompilationJobStatus']\n",
    "    \n",
    "    if status == 'INPROGRESS':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'STARTING':\n",
    "        print(\"*\", end='')\n",
    "    elif status == 'COMPLETED':\n",
    "        print(\"\\nJetson Xavier compilation completed!\")\n",
    "        break\n",
    "    elif status == 'FAILED':\n",
    "        print(\"\\nJetson Xavier compilation failed!\")\n",
    "        print(f\"Failure reason: {compile_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e2499f",
   "metadata": {},
   "source": [
    "### Compile for x86_64 CPU\n",
    "\n",
    "Target: Standard x86_64 architecture (Intel/AMD processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compilation job for x86_64 CPU\n",
    "compilation_job_name = \"class-x86-cpu-\" + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "print(f\"x86_64 compilation job: {compilation_job_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start compilation for x86_64 CPU\n",
    "create_response = sagemaker_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Input model configuration\n",
    "    InputConfig={\n",
    "        'S3Uri': compressed_model_path,\n",
    "        'DataInputConfig': classification_data_input_config,\n",
    "        'Framework': 'PYTORCH',\n",
    "        'FrameworkVersion': '1.8'\n",
    "    },\n",
    "    \n",
    "    # Output and target platform configuration\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket}/{project}/compilation_output',\n",
    "        'TargetPlatform': {\n",
    "            'Os': 'LINUX',\n",
    "            'Arch': 'X86_64'  # No GPU acceleration for CPU target\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"x86_64 CPU compilation job started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor x86_64 compilation progress\n",
    "print(\"Monitoring x86_64 compilation...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    compile_response = sagemaker_client.describe_compilation_job(\n",
    "        CompilationJobName=compilation_job_name\n",
    "    )\n",
    "    \n",
    "    status = compile_response['CompilationJobStatus']\n",
    "    \n",
    "    if status == 'INPROGRESS':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'STARTING':\n",
    "        print(\"*\", end='')\n",
    "    elif status == 'COMPLETED':\n",
    "        print(\"\\nx86_64 compilation completed!\")\n",
    "        break\n",
    "    elif status == 'FAILED':\n",
    "        print(\"\\nx86_64 compilation failed!\")\n",
    "        print(f\"Failure reason: {compile_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0821bee",
   "metadata": {},
   "source": [
    "### Compile for ARM64 CPU\n",
    "\n",
    "Target: ARM64 architecture without GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compilation job for ARM64 CPU\n",
    "compilation_arm_cpu = \"class-arm-cpu-\" + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "print(f\"ARM64 CPU compilation job: {compilation_arm_cpu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start compilation for ARM64 CPU\n",
    "create_arm_response = sagemaker_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_arm_cpu,\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Input model configuration\n",
    "    InputConfig={\n",
    "        'S3Uri': compressed_model_path,\n",
    "        'DataInputConfig': classification_data_input_config,\n",
    "        'Framework': 'PYTORCH',\n",
    "        'FrameworkVersion': '1.8'\n",
    "    },\n",
    "    \n",
    "    # Output and target platform configuration\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket}/{project}/compilation_output',\n",
    "        'TargetPlatform': {\n",
    "            'Os': 'LINUX',\n",
    "            'Arch': 'ARM64'  # ARM64 without GPU\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ARM64 CPU compilation job started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor ARM64 compilation progress\n",
    "print(\"Monitoring ARM64 compilation...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    create_arm_response = sagemaker_client.describe_compilation_job(\n",
    "        CompilationJobName=compilation_arm_cpu\n",
    "    )\n",
    "    \n",
    "    status = create_arm_response['CompilationJobStatus']\n",
    "    \n",
    "    if status == 'INPROGRESS':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'STARTING':\n",
    "        print(\"*\", end='')\n",
    "    elif status == 'COMPLETED':\n",
    "        print(\"\\nARM64 compilation completed!\")\n",
    "        break\n",
    "    elif status == 'FAILED':\n",
    "        print(\"\\nARM64 compilation failed!\")\n",
    "        print(f\"Failure reason: {create_arm_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b2b74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Model Compilation - Segmentation\n",
    "\n",
    "Compile the trained segmentation model for x86_64 CPU deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa3e0da",
   "metadata": {},
   "source": [
    "### Prepare Segmentation Model for Compilation\n",
    "\n",
    "Similar process as classification: extract and repackage the segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ab9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get segmentation model artifact location\n",
    "res_seg = sagemaker_client.describe_training_job(TrainingJobName=segmentation_training_job_name)\n",
    "seg_output_model_path = res_seg['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(f\"Segmentation model artifact: {seg_output_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda57cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse segmentation model S3 URI\n",
    "parsed_url = urlparse(seg_output_model_path)\n",
    "output_bucket = parsed_url.netloc\n",
    "output_key = parsed_url.path.lstrip('/')\n",
    "\n",
    "print(f\"S3 Bucket: {output_bucket}\")\n",
    "print(f\"S3 Key: {output_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download, extract, and repackage segmentation model\n",
    "path = \"./segmentation\"\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download segmentation model artifact\n",
    "input_tar_gz = os.path.join(path, 'model.tar.gz')\n",
    "s3_client.download_file(output_bucket, output_key, input_tar_gz)\n",
    "print(f\"Downloaded segmentation model to {input_tar_gz}\")\n",
    "\n",
    "# Extract the model archive\n",
    "extract_dir = os.path.join(path, 'extracted')\n",
    "Path(extract_dir).mkdir(parents=True, exist_ok=True)\n",
    "with tarfile.open(input_tar_gz, 'r:gz') as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "print(f\"Extracted contents to {extract_dir}\")\n",
    "\n",
    "# Find the mochi.pt model file\n",
    "model_file = os.path.join(extract_dir, 'mochi.pt')\n",
    "if not os.path.exists(model_file):\n",
    "    raise FileNotFoundError(\"mochi.pt file not found in segmentation model\")\n",
    "print(f\"Found segmentation model file: {model_file}\")\n",
    "\n",
    "# Extract input_shape from mochi.json\n",
    "mochi_json_path = os.path.join(extract_dir, 'mochi.json')\n",
    "if not os.path.exists(mochi_json_path):\n",
    "    raise FileNotFoundError(\"mochi.json file not found in segmentation model\")\n",
    "\n",
    "print(f\"Found mochi.json file: {mochi_json_path}\")\n",
    "with open(mochi_json_path, 'r') as f:\n",
    "    mochi_data = json.load(f)\n",
    "    input_shape = mochi_data['stages'][0]['input_shape']\n",
    "    print(f\"Extracted input_shape: {input_shape}\")\n",
    "    \n",
    "    # Extract height and width\n",
    "    height = input_shape[2]\n",
    "    width = input_shape[3]\n",
    "    \n",
    "    # Build tensor shape for DataInputConfig\n",
    "    tensor_shape = [1, 3, height, width]\n",
    "    segmentation_data_input_config = json.dumps({\"input_shape\": tensor_shape})\n",
    "    \n",
    "    print(f\"Height: {height}, Width: {width}\")\n",
    "    print(f\"Segmentation DataInputConfig: {segmentation_data_input_config}\")\n",
    "\n",
    "# Create new archive for compilation\n",
    "output_tar_gz = os.path.join(path, 'segmentation.tar.gz')\n",
    "with tarfile.open(output_tar_gz, \"w:gz\") as tar:\n",
    "    tar.add(model_file, arcname=os.path.basename(model_file))\n",
    "print(f\"Created compilation-ready archive: {output_tar_gz}\")\n",
    "\n",
    "# Upload repackaged segmentation model\n",
    "target_key = output_key.rsplit('/', 1)[0] + '/segmentation.tar.gz'\n",
    "s3_client.upload_file(output_tar_gz, output_bucket, target_key)\n",
    "print(f\"Uploaded to s3://{output_bucket}/{target_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707732a",
   "metadata": {},
   "source": [
    "### Compile Segmentation Model for x86_64 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497382f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segmentation compilation job\n",
    "compilation_job = \"seg-x86-cpu-\" + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "model_path = f\"s3://{output_bucket}/{target_key}\"\n",
    "\n",
    "print(f\"Segmentation compilation job: {compilation_job}\")\n",
    "print(f\"Model path: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfaed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start segmentation model compilation\n",
    "seg_x86_response = sagemaker_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job,\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Input configuration for segmentation model\n",
    "    InputConfig={\n",
    "        'S3Uri': model_path,\n",
    "        'DataInputConfig': segmentation_data_input_config,\n",
    "        'Framework': 'PYTORCH',\n",
    "        'FrameworkVersion': '1.8'\n",
    "    },\n",
    "    \n",
    "    # Output configuration\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket}/{project}/compilation_output',\n",
    "        'TargetPlatform': {\n",
    "            'Os': 'LINUX',\n",
    "            'Arch': 'X86_64'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Segmentation x86_64 compilation job started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d4701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor segmentation compilation progress\n",
    "print(\"Monitoring segmentation compilation...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    create_response = sagemaker_client.describe_compilation_job(\n",
    "        CompilationJobName=compilation_job\n",
    "    )\n",
    "    \n",
    "    status = create_response['CompilationJobStatus']\n",
    "    \n",
    "    if status == 'INPROGRESS':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'STARTING':\n",
    "        print(\"*\", end='')\n",
    "    elif status == 'COMPLETED':\n",
    "        print(\"\\nSegmentation compilation completed!\")\n",
    "        break\n",
    "    elif status == 'FAILED':\n",
    "        print(\"\\nSegmentation compilation failed!\")\n",
    "        print(f\"Failure reason: {create_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seg_arm64_header",
   "metadata": {},
   "source": [
    "### Compile Segmentation Model for ARM64 CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seg_arm64_job",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segmentation ARM64 compilation job\n",
    "compilation_seg_arm = \"seg-arm-cpu-\" + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "print(f\"Segmentation ARM64 compilation job: {compilation_seg_arm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seg_arm64_compile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start segmentation ARM64 compilation\n",
    "seg_arm_response = sagemaker_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_seg_arm,\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Input configuration\n",
    "    InputConfig={\n",
    "        'S3Uri': model_path,\n",
    "        'DataInputConfig': segmentation_data_input_config,\n",
    "        'Framework': 'PYTORCH',\n",
    "        'FrameworkVersion': '1.8'\n",
    "    },\n",
    "    \n",
    "    # Output configuration for ARM64\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket}/{project}/compilation_output',\n",
    "        'TargetPlatform': {\n",
    "            'Os': 'LINUX',\n",
    "            'Arch': 'ARM64'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Segmentation ARM64 compilation job started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seg_arm64_monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor segmentation ARM64 compilation\n",
    "print(\"Monitoring segmentation ARM64 compilation...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    seg_arm_response = sagemaker_client.describe_compilation_job(\n",
    "        CompilationJobName=compilation_seg_arm\n",
    "    )\n",
    "    \n",
    "    status = seg_arm_response['CompilationJobStatus']\n",
    "    \n",
    "    if status == 'INPROGRESS':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'STARTING':\n",
    "        print(\"*\", end='')\n",
    "    elif status == 'COMPLETED':\n",
    "        print(\"\\nSegmentation ARM64 compilation completed!\")\n",
    "        break\n",
    "    elif status == 'FAILED':\n",
    "        print(\"\\nSegmentation ARM64 compilation failed!\")\n",
    "        print(f\"Failure reason: {seg_arm_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seg_jetson_header",
   "metadata": {},
   "source": [
    "### Compile Segmentation Model for Jetson Xavier\n",
    "\n",
    "Target: ARM64 architecture with NVIDIA GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seg_jetson_job",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segmentation Jetson compilation job\n",
    "compilation_seg_jetson = \"seg-xavier-gpu-\" + datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "\n",
    "print(f\"Segmentation Jetson Xavier compilation job: {compilation_seg_jetson}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seg_jetson_compile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start segmentation Jetson Xavier compilation\n",
    "seg_jetson_response = sagemaker_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_seg_jetson,\n",
    "    RoleArn=sm_role_arn,\n",
    "    \n",
    "    # Input configuration\n",
    "    InputConfig={\n",
    "        'S3Uri': model_path,\n",
    "        'DataInputConfig': segmentation_data_input_config,\n",
    "        'Framework': 'PYTORCH',\n",
    "        'FrameworkVersion': '1.8'\n",
    "    },\n",
    "    \n",
    "    # Output configuration for Jetson Xavier\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket}/{project}/compilation_output',\n",
    "        'TargetPlatform': {\n",
    "            'Os': 'LINUX',\n",
    "            'Arch': 'ARM64',\n",
    "            'Accelerator': 'NVIDIA'\n",
    "        },\n",
    "        # Jetson Xavier specific compiler options\n",
    "        'CompilerOptions': '{\"cuda-ver\": \"10.2\",\"gpu-code\": \"sm_72\",\"trt-ver\": \"8.2.1\"}'\n",
    "    },\n",
    "    \n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 3600\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Segmentation Jetson Xavier compilation job started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seg_jetson_monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor segmentation Jetson compilation\n",
    "print(\"Monitoring segmentation Jetson compilation...\")\n",
    "print(\"Status: \", end=\"\")\n",
    "\n",
    "while True:\n",
    "    seg_jetson_response = sagemaker_client.describe_compilation_job(\n",
    "        CompilationJobName=compilation_seg_jetson\n",
    "    )\n",
    "    \n",
    "    status = seg_jetson_response['CompilationJobStatus']\n",
    "    \n",
    "    if status == 'INPROGRESS':\n",
    "        print(\".\", end='')\n",
    "    elif status == 'STARTING':\n",
    "        print(\"*\", end='')\n",
    "    elif status == 'COMPLETED':\n",
    "        print(\"\\nSegmentation Jetson compilation completed!\")\n",
    "        break\n",
    "    elif status == 'FAILED':\n",
    "        print(\"\\nSegmentation Jetson compilation failed!\")\n",
    "        print(f\"Failure reason: {seg_jetson_response.get('FailureReason', 'Unknown')}\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"?\", end='')\n",
    "    \n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c17b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "1. **Trained Models:**\n",
    "   - Classification model for binary defect detection\n",
    "   - Segmentation model for pixel-level defect localization\n",
    "\n",
    "2. **Compiled Models for Multiple Targets:**\n",
    "   - **Classification:** Jetson Xavier (ARM64 + GPU), x86_64 CPU, ARM64 CPU\n",
    "   - **Segmentation:** x86_64 CPU, ARM64 CPU, Jetson Xavier (ARM64 + GPU)\n",
    "\n",
    "3. **Prepared for DDA Deployment:**\n",
    "   - Models are optimized for edge deployment\n",
    "   - Compatible with AWS IoT Greengrass\n",
    "   - Ready for integration with DDA application\n",
    "\n",
    "**Next Steps:**\n",
    "- Download compiled models from S3\n",
    "- Deploy to target edge devices using DDA\n",
    "- Configure inference parameters\n",
    "- Test with production data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
