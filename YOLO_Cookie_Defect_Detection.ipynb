{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Cookie Defect Detection\n",
    "\n",
    "This notebook trains and compiles YOLOv8 models for cookie defect detection using Amazon SageMaker.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a complete pipeline for:\n",
    "1. Environment setup and configuration\n",
    "2. Dataset acquisition and preparation\n",
    "3. Format conversion (Lookout for Vision \u2192 YOLO)\n",
    "4. Model training using YOLOv8\n",
    "5. Model compilation for multiple target platforms\n",
    "6. Model comparison and validation\n",
    "\n",
    "**Supported Model Types:**\n",
    "- YOLO Object Detection (bounding boxes)\n",
    "- YOLO Instance Segmentation (polygon masks)\n",
    "\n",
    "**Target Platforms:**\n",
    "- Jetson Xavier GPU\n",
    "- x86_64 CPU\n",
    "- ARM64 CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Initialize the SageMaker environment and configure project settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Library Imports\n",
    "\n",
    "Import required libraries for SageMaker operations, S3 access, and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS and SageMaker libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Standard libraries\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Display library versions\n",
    "print(f\"SageMaker SDK version: {sagemaker.__version__}\")\n",
    "print(f\"Boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 SageMaker Session Initialization\n",
    "\n",
    "Create a SageMaker session and retrieve the default S3 bucket and AWS region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get default S3 bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Get AWS region\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Get execution role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\u2705 SageMaker session initialized\")\n",
    "print(f\"   Region: {region}\")\n",
    "print(f\"   Default S3 bucket: {default_bucket}\")\n",
    "print(f\"   Execution role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Project Configuration and S3 Folder Structure\n",
    "\n",
    "Define project parameters and create the S3 folder structure for outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project configuration\n",
    "project_name = \"yolo-cookie-defect-detection\"\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Define S3 folder structure\n",
    "s3_prefix = f\"{project_name}/{timestamp}\"\n",
    "s3_paths = {\n",
    "    'training_output': f\"s3://{default_bucket}/{s3_prefix}/training-output\",\n",
    "    'compilation_output': f\"s3://{default_bucket}/{s3_prefix}/compilation-output\",\n",
    "    'dataset': f\"s3://{default_bucket}/{s3_prefix}/dataset\",\n",
    "    'models': f\"s3://{default_bucket}/{s3_prefix}/models\"\n",
    "}\n",
    "\n",
    "# Create S3 client\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "# Create S3 folders by uploading empty marker files\n",
    "for folder_name, s3_path in s3_paths.items():\n",
    "    # Extract bucket and key from S3 URI\n",
    "    s3_uri_parts = s3_path.replace('s3://', '').split('/', 1)\n",
    "    bucket = s3_uri_parts[0]\n",
    "    key = s3_uri_parts[1] + '/.folder_marker'\n",
    "    \n",
    "    try:\n",
    "        # Create folder marker\n",
    "        s3_client.put_object(Bucket=bucket, Key=key, Body=b'')\n",
    "        print(f\"\u2705 Created S3 folder: {s3_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Failed to create S3 folder {folder_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Display S3 paths\n",
    "print(\"\\n\ud83d\udcc1 S3 Folder Structure:\")\n",
    "for folder_name, s3_path in s3_paths.items():\n",
    "    print(f\"   {folder_name}: {s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Acquisition\n",
    "\n",
    "Download and prepare the cookie dataset from the amazon-lookout-for-vision repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Clone Repository\n",
    "\n",
    "Clone the amazon-lookout-for-vision repository from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Repository URL\n",
    "repo_url = \"https://github.com/aws-samples/amazon-lookout-for-vision.git\"\n",
    "repo_dir = \"amazon-lookout-for-vision\"\n",
    "\n",
    "# Remove existing repository if present\n",
    "if os.path.exists(repo_dir):\n",
    "    print(f\"\ud83d\uddd1\ufe0f  Removing existing repository: {repo_dir}\")\n",
    "    shutil.rmtree(repo_dir)\n",
    "\n",
    "# Clone repository\n",
    "print(f\"\ud83d\udce5 Cloning repository: {repo_url}\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"clone\", repo_url, repo_dir],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(f\"\u2705 Repository cloned successfully to: {repo_dir}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"\u274c Failed to clone repository: {e.stderr}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract Cookie Dataset\n",
    "\n",
    "Copy the cookie-dataset folder to the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source and destination paths\n",
    "source_dataset = os.path.join(repo_dir, \"cookie-dataset\")\n",
    "dest_dataset = \"cookie-dataset\"\n",
    "\n",
    "# Remove existing dataset if present\n",
    "if os.path.exists(dest_dataset):\n",
    "    print(f\"\ud83d\uddd1\ufe0f  Removing existing dataset: {dest_dataset}\")\n",
    "    shutil.rmtree(dest_dataset)\n",
    "\n",
    "# Copy dataset\n",
    "print(f\"\ud83d\udce6 Extracting cookie dataset from: {source_dataset}\")\n",
    "try:\n",
    "    if not os.path.exists(source_dataset):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cookie dataset not found at: {source_dataset}\\n\"\n",
    "            f\"Expected location: {os.path.abspath(source_dataset)}\\n\"\n",
    "            f\"Available directories in repo: {os.listdir(repo_dir) if os.path.exists(repo_dir) else 'Repository not found'}\"\n",
    "        )\n",
    "    \n",
    "    shutil.copytree(source_dataset, dest_dataset)\n",
    "    print(f\"\u2705 Dataset extracted successfully to: {dest_dataset}\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Failed to extract dataset: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Validate Dataset Structure\n",
    "\n",
    "Verify the presence of required files and count images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected dataset structure\n",
    "dataset_structure = {\n",
    "    'training_images': os.path.join(dest_dataset, 'dataset-files', 'training-images'),\n",
    "    'mask_images': os.path.join(dest_dataset, 'dataset-files', 'mask-images'),\n",
    "    'manifests': os.path.join(dest_dataset, 'dataset-files', 'manifests')\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udd0d Validating dataset structure...\\n\")\n",
    "\n",
    "# Validate each component\n",
    "validation_passed = True\n",
    "for component_name, component_path in dataset_structure.items():\n",
    "    if not os.path.exists(component_path):\n",
    "        print(f\"\u274c Missing: {component_name} at {component_path}\")\n",
    "        validation_passed = False\n",
    "    else:\n",
    "        print(f\"\u2705 Found: {component_name} at {component_path}\")\n",
    "\n",
    "if not validation_passed:\n",
    "    raise ValueError(\n",
    "        f\"Invalid dataset structure.\\n\"\n",
    "        f\"Missing required components.\\n\"\n",
    "        f\"Expected structure:\\n\"\n",
    "        f\"  - {dataset_structure['training_images']}\\n\"\n",
    "        f\"  - {dataset_structure['mask_images']}\\n\"\n",
    "        f\"  - {dataset_structure['manifests']}\"\n",
    "    )\n",
    "\n",
    "# Count images\n",
    "print(\"\\n\ud83d\udcca Dataset Statistics:\")\n",
    "\n",
    "# Count training images\n",
    "training_images = [f for f in os.listdir(dataset_structure['training_images']) \n",
    "                   if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "print(f\"   Training images: {len(training_images)}\")\n",
    "\n",
    "# Count mask images\n",
    "mask_images = [f for f in os.listdir(dataset_structure['mask_images']) \n",
    "               if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "print(f\"   Mask images: {len(mask_images)}\")\n",
    "\n",
    "# List manifest files\n",
    "manifest_files = [f for f in os.listdir(dataset_structure['manifests']) \n",
    "                  if f.endswith('.json') or f.endswith('.jsonl')]\n",
    "print(f\"   Manifest files: {len(manifest_files)}\")\n",
    "if manifest_files:\n",
    "    for manifest in manifest_files:\n",
    "        print(f\"      - {manifest}\")\n",
    "\n",
    "print(\"\\n\u2705 Dataset validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Cleanup Cloned Repository\n",
    "\n",
    "Remove the cloned repository to save space (keeping only the extracted dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cloned repository\n",
    "if os.path.exists(repo_dir):\n",
    "    print(f\"\ud83d\uddd1\ufe0f  Cleaning up: Removing {repo_dir}\")\n",
    "    try:\n",
    "        shutil.rmtree(repo_dir)\n",
    "        print(f\"\u2705 Repository removed successfully\")\n",
    "        print(f\"   Dataset retained at: {dest_dataset}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Warning: Failed to remove repository: {str(e)}\")\n",
    "        print(f\"   You may need to manually delete: {repo_dir}\")\n",
    "else:\n",
    "    print(f\"\u2139\ufe0f  Repository directory not found: {repo_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Format Conversion\n",
    "\n",
    "Convert Lookout for Vision annotations to YOLO format (detection and segmentation).\n",
    "\n",
    "*This section will be implemented in subsequent tasks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import Format Converter\n",
    "\n",
    "Import the YOLO format converter module with helper functions for converting Lookout for Vision annotations to YOLO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import format converter functions\n",
    "from yolo_format_converter import (\n",
    "    extract_bounding_boxes,\n",
    "    normalize_coordinates,\n",
    "    convert_to_yolo_format,\n",
    "    read_manifest,\n",
    "    write_yolo_annotations,\n",
    "    create_data_yaml\n",
    ")\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"\u2705 Format converter imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Process Manifest and Extract Bounding Boxes\n",
    "\n",
    "Read the Lookout for Vision manifest file and extract bounding boxes from segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "manifest_path = 'cookie-dataset/dataset-files/manifests/output.manifest'\n",
    "mask_images_dir = 'cookie-dataset/dataset-files/mask-images'\n",
    "training_images_dir = 'cookie-dataset/dataset-files/training-images'\n",
    "\n",
    "# Read manifest file\n",
    "print(\"\ud83d\udcd6 Reading manifest file...\")\n",
    "manifest_records = read_manifest(manifest_path)\n",
    "print(f\"\u2705 Loaded {len(manifest_records)} records from manifest\")\n",
    "\n",
    "# Process each record and extract bounding boxes\n",
    "annotations = {}  # Dictionary: image_filename -> list of YOLO annotation lines\n",
    "\n",
    "print(\"\\n\ud83d\udd0d Extracting bounding boxes from masks...\")\n",
    "for i, record in enumerate(manifest_records):\n",
    "    # Get image filename from source-ref\n",
    "    source_ref = record['source-ref']\n",
    "    image_filename = os.path.basename(source_ref)\n",
    "    \n",
    "    # Get anomaly label (0=normal, 1=anomaly)\n",
    "    class_id = record['anomaly-label']\n",
    "    \n",
    "    # If this is an anomaly, extract bounding boxes from mask\n",
    "    if class_id == 1:\n",
    "        # Find corresponding mask file\n",
    "        mask_filename = image_filename.replace('.jpg', '_mask.png')\n",
    "        mask_path = os.path.join(mask_images_dir, mask_filename)\n",
    "        \n",
    "        if os.path.exists(mask_path):\n",
    "            # Extract bounding boxes\n",
    "            bboxes = extract_bounding_boxes(mask_path)\n",
    "            \n",
    "            # Get image dimensions\n",
    "            img = cv2.imread(os.path.join(training_images_dir, image_filename))\n",
    "            img_height, img_width = img.shape[:2]\n",
    "            \n",
    "            # Convert each bbox to YOLO format\n",
    "            yolo_annotations = []\n",
    "            for bbox in bboxes:\n",
    "                # Normalize coordinates\n",
    "                normalized_bbox = normalize_coordinates(bbox, img_width, img_height)\n",
    "                \n",
    "                # Convert to YOLO format string\n",
    "                yolo_line = convert_to_yolo_format(normalized_bbox, class_id)\n",
    "                yolo_annotations.append(yolo_line)\n",
    "            \n",
    "            annotations[image_filename] = yolo_annotations\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Processed {i + 1}/{len(manifest_records)} images...\")\n",
    "        else:\n",
    "            print(f\"\u26a0\ufe0f  Warning: Mask file not found for {image_filename}\")\n",
    "    else:\n",
    "        # Normal image - no annotations needed (or empty annotation file)\n",
    "        annotations[image_filename] = []\n",
    "\n",
    "print(f\"\\n\u2705 Extracted bounding boxes for {len(annotations)} images\")\n",
    "print(f\"   Images with defects: {sum(1 for v in annotations.values() if len(v) > 0)}\")\n",
    "print(f\"   Normal images: {sum(1 for v in annotations.values() if len(v) == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Write YOLO Detection Annotations\n",
    "\n",
    "Write the YOLO format annotation files (.txt) for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for annotations\n",
    "annotations_dir = 'yolo-dataset/labels/train'\n",
    "os.makedirs(annotations_dir, exist_ok=True)\n",
    "\n",
    "# Write YOLO annotations\n",
    "print(\"\ud83d\udcdd Writing YOLO annotation files...\")\n",
    "write_yolo_annotations(annotations, annotations_dir)\n",
    "\n",
    "# Verify annotations were written\n",
    "annotation_files = list(Path(annotations_dir).glob('*.txt'))\n",
    "print(f\"\u2705 Wrote {len(annotation_files)} annotation files to {annotations_dir}\")\n",
    "\n",
    "# Display sample annotation\n",
    "if annotation_files:\n",
    "    sample_file = annotation_files[0]\n",
    "    print(f\"\\n\ud83d\udcc4 Sample annotation file: {sample_file.name}\")\n",
    "    with open(sample_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        if content:\n",
    "            print(f\"   Content: {content.strip()}\")\n",
    "        else:\n",
    "            print(\"   (Empty - normal image with no defects)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create data.yaml Configuration\n",
    "\n",
    "Generate the YOLO dataset configuration file specifying class names and dataset paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class names\n",
    "class_names = ['normal', 'anomaly']\n",
    "\n",
    "# Create data.yaml\n",
    "data_yaml_path = 'yolo-dataset/data.yaml'\n",
    "print(\"\ud83d\udcdd Creating data.yaml configuration...\")\n",
    "create_data_yaml(class_names, data_yaml_path)\n",
    "\n",
    "# Display the configuration\n",
    "print(f\"\u2705 Created {data_yaml_path}\")\n",
    "print(\"\\n\ud83d\udcc4 Configuration content:\")\n",
    "with open(data_yaml_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 YOLO Segmentation Format Conversion (Optional)\n",
    "\n",
    "This section converts the Lookout for Vision annotations to YOLO segmentation format.\n",
    "YOLO segmentation uses polygon coordinates instead of bounding boxes for more precise defect localization.\n",
    "\n",
    "**Note:** Skip this section if you only want to train detection models. Segmentation models require more computational resources but provide pixel-level accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Process Manifest and Extract Polygons\n",
    "\n",
    "Extract polygon coordinates from segmentation masks using contour detection and Douglas-Peucker approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import segmentation functions\n",
    "from yolo_format_converter import (\n",
    "    extract_polygons,\n",
    "    approximate_polygon,\n",
    "    convert_to_yolo_segment_format\n",
    ")\n",
    "\n",
    "# Define paths for segmentation\n",
    "manifest_path_seg = 'cookie-dataset/dataset-files/manifests/output.manifest'\n",
    "mask_dir_seg = 'cookie-dataset/dataset-files/mask-images'\n",
    "output_dir_seg = 'yolo-dataset-segmentation/labels/train'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir_seg, exist_ok=True)\n",
    "\n",
    "# Read manifest\n",
    "manifest_records_seg = read_manifest(manifest_path_seg)\n",
    "\n",
    "print(f\"\\u2705 Loaded {len(manifest_records_seg)} records from manifest\")\n",
    "print(f\"\\u2705 Output directory: {output_dir_seg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each record and extract polygons\n",
    "segmentation_annotations = {}\n",
    "epsilon = 0.01  # Douglas-Peucker approximation parameter\n",
    "\n",
    "for record in manifest_records_seg:\n",
    "    # Get image filename from source-ref\n",
    "    source_ref = record['source-ref']\n",
    "    image_filename = os.path.basename(source_ref)\n",
    "    \n",
    "    # Get class label\n",
    "    class_id = record['anomaly-label']\n",
    "    \n",
    "    # Skip normal images (no defects to segment)\n",
    "    if class_id == 0:\n",
    "        segmentation_annotations[image_filename] = []\n",
    "        continue\n",
    "    \n",
    "    # Get corresponding mask file\n",
    "    mask_filename = image_filename.replace('.jpg', '_mask.png')\n",
    "    mask_path = os.path.join(mask_dir_seg, mask_filename)\n",
    "    \n",
    "    if not os.path.exists(mask_path):\n",
    "        print(f\"\\u26a0\\ufe0f  Warning: Mask not found for {image_filename}\")\n",
    "        segmentation_annotations[image_filename] = []\n",
    "        continue\n",
    "    \n",
    "    # Extract polygons from mask\n",
    "    polygons = extract_polygons(mask_path)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    import cv2\n",
    "    img = cv2.imread(os.path.join('cookie-dataset/dataset-files/training-images', image_filename))\n",
    "    img_height, img_width = img.shape[:2]\n",
    "    \n",
    "    # Convert each polygon to YOLO segmentation format\n",
    "    annotation_lines = []\n",
    "    for polygon in polygons:\n",
    "        # Approximate polygon to reduce number of points\n",
    "        approx_polygon = approximate_polygon(polygon, epsilon=epsilon)\n",
    "        \n",
    "        # Convert to YOLO format\n",
    "        yolo_line = convert_to_yolo_segment_format(\n",
    "            approx_polygon, class_id, img_width, img_height\n",
    "        )\n",
    "        annotation_lines.append(yolo_line)\n",
    "    \n",
    "    segmentation_annotations[image_filename] = annotation_lines\n",
    "\n",
    "print(f\"\\u2705 Processed {len(segmentation_annotations)} images\")\n",
    "print(f\"\\u2705 Found {sum(len(lines) for lines in segmentation_annotations.values())} polygon annotations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Write YOLO Segmentation Annotations\n",
    "\n",
    "Write polygon annotations to .txt files in YOLO segmentation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write segmentation annotations to files\n",
    "write_yolo_annotations(segmentation_annotations, output_dir_seg)\n",
    "\n",
    "# Count annotation files\n",
    "annotation_files = [f for f in os.listdir(output_dir_seg) if f.endswith('.txt')]\n",
    "\n",
    "print(f\"\\u2705 Created {len(annotation_files)} annotation files\")\n",
    "print(f\"\\u2705 Annotations saved to: {output_dir_seg}\")\n",
    "\n",
    "# Display sample annotation\n",
    "if annotation_files:\n",
    "    sample_file = os.path.join(output_dir_seg, annotation_files[0])\n",
    "    with open(sample_file, 'r') as f:\n",
    "        sample_content = f.read()\n",
    "    print(f\"\\n\\ud83d\\udd0d Sample annotation ({annotation_files[0]}):\")\n",
    "    print(sample_content[:200] + '...' if len(sample_content) > 200 else sample_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3 Create data.yaml for Segmentation\n",
    "\n",
    "Generate the YOLO dataset configuration file for segmentation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.yaml for segmentation\n",
    "data_yaml_path_seg = 'yolo-dataset-segmentation/data.yaml'\n",
    "class_names_seg = ['normal', 'anomaly']\n",
    "\n",
    "create_data_yaml(\n",
    "    class_names=class_names_seg,\n",
    "    output_path=data_yaml_path_seg,\n",
    "    train_path='images/train',\n",
    "    val_path='images/val'\n",
    ")\n",
    "\n",
    "print(f\"\\u2705 Created data.yaml for segmentation: {data_yaml_path_seg}\")\n",
    "\n",
    "# Display contents\n",
    "with open(data_yaml_path_seg, 'r') as f:\n",
    "    yaml_content = f.read()\n",
    "\n",
    "print(f\"\\n\\ud83d\\udd0d data.yaml contents:\")\n",
    "print(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.4 Copy Training Images for Segmentation\n",
    "\n",
    "Copy training images to the segmentation dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create images directory for segmentation\n",
    "images_dir_seg = 'yolo-dataset-segmentation/images/train'\n",
    "os.makedirs(images_dir_seg, exist_ok=True)\n",
    "\n",
    "# Copy all training images\n",
    "source_images_dir = 'cookie-dataset/dataset-files/training-images'\n",
    "image_files = [f for f in os.listdir(source_images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "for image_file in image_files:\n",
    "    src = os.path.join(source_images_dir, image_file)\n",
    "    dst = os.path.join(images_dir_seg, image_file)\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "print(f\"\\u2705 Copied {len(image_files)} training images to {images_dir_seg}\")\n",
    "print(f\"\\n\\ud83c\\udf89 YOLO segmentation dataset ready!\")\n",
    "print(f\"   Dataset location: yolo-dataset-segmentation/\")\n",
    "print(f\"   Images: {len(image_files)}\")\n",
    "print(f\"   Annotations: {len(annotation_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Copy Training Images\n",
    "\n",
    "Copy training images to the YOLO dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create images directory\n",
    "images_dir = 'yolo-dataset/images/train'\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# Copy training images\n",
    "print(\"\ud83d\udcc1 Copying training images...\")\n",
    "training_images = list(Path(training_images_dir).glob('*.jpg'))\n",
    "\n",
    "for img_path in training_images:\n",
    "    dest_path = os.path.join(images_dir, img_path.name)\n",
    "    shutil.copy2(img_path, dest_path)\n",
    "\n",
    "print(f\"\u2705 Copied {len(training_images)} images to {images_dir}\")\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"\\n\ud83d\udcca YOLO Dataset Structure:\")\n",
    "print(f\"   Images: {len(list(Path(images_dir).glob('*.jpg')))} files\")\n",
    "print(f\"   Labels: {len(list(Path(annotations_dir).glob('*.txt')))} files\")\n",
    "print(f\"   Config: data.yaml\")\n",
    "print(\"\\n\u2705 YOLO detection format conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. S3 Upload for Training Data\n",
    "\n",
    "Upload the YOLO-formatted dataset to S3 for SageMaker training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Upload Training Images to S3\n",
    "\n",
    "Upload all training images to the S3 training-images folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Define S3 paths for dataset\n",
    "s3_dataset_prefix = f\"{s3_prefix}/dataset\"\n",
    "s3_images_prefix = f\"{s3_dataset_prefix}/images/train\"\n",
    "s3_labels_prefix = f\"{s3_dataset_prefix}/labels/train\"\n",
    "\n",
    "# Upload training images\n",
    "print(\"\ud83d\udce4 Uploading training images to S3...\")\n",
    "images_to_upload = list(Path(images_dir).glob('*.jpg'))\n",
    "uploaded_images = 0\n",
    "failed_uploads = []\n",
    "\n",
    "for img_path in images_to_upload:\n",
    "    s3_key = f\"{s3_images_prefix}/{img_path.name}\"\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(\n",
    "            str(img_path),\n",
    "            default_bucket,\n",
    "            s3_key\n",
    "        )\n",
    "        uploaded_images += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if uploaded_images % 10 == 0:\n",
    "            print(f\"  Uploaded {uploaded_images}/{len(images_to_upload)} images...\")\n",
    "            \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_msg = e.response['Error']['Message']\n",
    "        failed_uploads.append({\n",
    "            'file': str(img_path),\n",
    "            'error': f\"{error_code}: {error_msg}\",\n",
    "            's3_destination': f\"s3://{default_bucket}/{s3_key}\"\n",
    "        })\n",
    "        print(f\"\u274c S3 upload failed: {error_code}\")\n",
    "        print(f\"   File: {img_path}\")\n",
    "        print(f\"   Destination: s3://{default_bucket}/{s3_key}\")\n",
    "        print(f\"   Error: {error_msg}\")\n",
    "\n",
    "# Display results\n",
    "if failed_uploads:\n",
    "    print(f\"\\n\u26a0\ufe0f  Upload completed with errors:\")\n",
    "    print(f\"   Successfully uploaded: {uploaded_images}/{len(images_to_upload)} images\")\n",
    "    print(f\"   Failed uploads: {len(failed_uploads)}\")\n",
    "    for failure in failed_uploads:\n",
    "        print(f\"      - {failure['file']}: {failure['error']}\")\n",
    "else:\n",
    "    print(f\"\\n\u2705 Successfully uploaded {uploaded_images} training images\")\n",
    "    print(f\"   S3 location: s3://{default_bucket}/{s3_images_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Upload YOLO Annotations to S3\n",
    "\n",
    "Upload all YOLO .txt annotation files to the S3 annotations folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload annotation files\n",
    "print(\"\ud83d\udce4 Uploading YOLO annotations to S3...\")\n",
    "annotations_to_upload = list(Path(annotations_dir).glob('*.txt'))\n",
    "uploaded_annotations = 0\n",
    "failed_annotation_uploads = []\n",
    "\n",
    "for ann_path in annotations_to_upload:\n",
    "    s3_key = f\"{s3_labels_prefix}/{ann_path.name}\"\n",
    "    \n",
    "    try:\n",
    "        s3_client.upload_file(\n",
    "            str(ann_path),\n",
    "            default_bucket,\n",
    "            s3_key\n",
    "        )\n",
    "        uploaded_annotations += 1\n",
    "        \n",
    "        # Progress indicator\n",
    "        if uploaded_annotations % 10 == 0:\n",
    "            print(f\"  Uploaded {uploaded_annotations}/{len(annotations_to_upload)} annotations...\")\n",
    "            \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_msg = e.response['Error']['Message']\n",
    "        failed_annotation_uploads.append({\n",
    "            'file': str(ann_path),\n",
    "            'error': f\"{error_code}: {error_msg}\",\n",
    "            's3_destination': f\"s3://{default_bucket}/{s3_key}\"\n",
    "        })\n",
    "        print(f\"\u274c S3 upload failed: {error_code}\")\n",
    "        print(f\"   File: {ann_path}\")\n",
    "        print(f\"   Destination: s3://{default_bucket}/{s3_key}\")\n",
    "        print(f\"   Error: {error_msg}\")\n",
    "\n",
    "# Display results\n",
    "if failed_annotation_uploads:\n",
    "    print(f\"\\n\u26a0\ufe0f  Upload completed with errors:\")\n",
    "    print(f\"   Successfully uploaded: {uploaded_annotations}/{len(annotations_to_upload)} annotations\")\n",
    "    print(f\"   Failed uploads: {len(failed_annotation_uploads)}\")\n",
    "    for failure in failed_annotation_uploads:\n",
    "        print(f\"      - {failure['file']}: {failure['error']}\")\n",
    "else:\n",
    "    print(f\"\\n\u2705 Successfully uploaded {uploaded_annotations} annotation files\")\n",
    "    print(f\"   S3 location: s3://{default_bucket}/{s3_labels_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Upload data.yaml Configuration to S3\n",
    "\n",
    "Upload the YOLO dataset configuration file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data.yaml\n",
    "print(\"\ud83d\udce4 Uploading data.yaml to S3...\")\n",
    "data_yaml_s3_key = f\"{s3_dataset_prefix}/data.yaml\"\n",
    "\n",
    "try:\n",
    "    s3_client.upload_file(\n",
    "        data_yaml_path,\n",
    "        default_bucket,\n",
    "        data_yaml_s3_key\n",
    "    )\n",
    "    print(f\"\u2705 Successfully uploaded data.yaml\")\n",
    "    print(f\"   S3 location: s3://{default_bucket}/{data_yaml_s3_key}\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    error_msg = e.response['Error']['Message']\n",
    "    print(f\"\u274c S3 upload failed: {error_code}\")\n",
    "    print(f\"   File: {data_yaml_path}\")\n",
    "    print(f\"   Destination: s3://{default_bucket}/{data_yaml_s3_key}\")\n",
    "    print(f\"   Error: {error_msg}\")\n",
    "    raise Exception(\n",
    "        f\"Failed to upload data.yaml to S3.\\n\"\n",
    "        f\"File: {data_yaml_path}\\n\"\n",
    "        f\"Bucket: {default_bucket}\\n\"\n",
    "        f\"Key: {data_yaml_s3_key}\\n\"\n",
    "        f\"Error: {error_code} - {error_msg}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Verify Uploads and Display S3 URIs\n",
    "\n",
    "Verify all files were uploaded successfully and display the S3 URIs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploads by listing S3 objects\n",
    "print(\"\ud83d\udd0d Verifying S3 uploads...\\n\")\n",
    "\n",
    "# Check images\n",
    "try:\n",
    "    images_response = s3_client.list_objects_v2(\n",
    "        Bucket=default_bucket,\n",
    "        Prefix=s3_images_prefix,\n",
    "        MaxKeys=1000\n",
    "    )\n",
    "    s3_image_count = images_response.get('KeyCount', 0)\n",
    "    print(f\"\u2705 Images in S3: {s3_image_count} files\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    print(f\"\u274c Failed to list images in S3: {e.response['Error']['Code']}\")\n",
    "    print(f\"   Bucket: {default_bucket}\")\n",
    "    print(f\"   Prefix: {s3_images_prefix}\")\n",
    "    s3_image_count = 0\n",
    "\n",
    "# Check annotations\n",
    "try:\n",
    "    labels_response = s3_client.list_objects_v2(\n",
    "        Bucket=default_bucket,\n",
    "        Prefix=s3_labels_prefix,\n",
    "        MaxKeys=1000\n",
    "    )\n",
    "    s3_label_count = labels_response.get('KeyCount', 0)\n",
    "    print(f\"\u2705 Annotations in S3: {s3_label_count} files\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    print(f\"\u274c Failed to list annotations in S3: {e.response['Error']['Code']}\")\n",
    "    print(f\"   Bucket: {default_bucket}\")\n",
    "    print(f\"   Prefix: {s3_labels_prefix}\")\n",
    "    s3_label_count = 0\n",
    "\n",
    "# Check data.yaml\n",
    "try:\n",
    "    s3_client.head_object(\n",
    "        Bucket=default_bucket,\n",
    "        Key=data_yaml_s3_key\n",
    "    )\n",
    "    print(f\"\u2705 data.yaml exists in S3\")\n",
    "    data_yaml_exists = True\n",
    "    \n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == '404':\n",
    "        print(f\"\u274c data.yaml not found in S3\")\n",
    "        print(f\"   Expected location: s3://{default_bucket}/{data_yaml_s3_key}\")\n",
    "    else:\n",
    "        print(f\"\u274c Failed to check data.yaml: {e.response['Error']['Code']}\")\n",
    "    data_yaml_exists = False\n",
    "\n",
    "# Display S3 URIs for training\n",
    "print(\"\\n\ud83d\udccd S3 Dataset URIs for Training:\")\n",
    "print(f\"   Dataset root: s3://{default_bucket}/{s3_dataset_prefix}/\")\n",
    "print(f\"   Training images: s3://{default_bucket}/{s3_images_prefix}/\")\n",
    "print(f\"   Annotations: s3://{default_bucket}/{s3_labels_prefix}/\")\n",
    "print(f\"   Configuration: s3://{default_bucket}/{data_yaml_s3_key}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\ud83d\udcca Upload Summary:\")\n",
    "print(f\"   Local images: {len(images_to_upload)}\")\n",
    "print(f\"   S3 images: {s3_image_count}\")\n",
    "print(f\"   Local annotations: {len(annotations_to_upload)}\")\n",
    "print(f\"   S3 annotations: {s3_label_count}\")\n",
    "print(f\"   data.yaml: {'Present' if data_yaml_exists else 'Missing'}\")\n",
    "\n",
    "# Check for mismatches\n",
    "if s3_image_count != len(images_to_upload):\n",
    "    print(f\"\\n\u26a0\ufe0f  Warning: Image count mismatch!\")\n",
    "    print(f\"   Expected {len(images_to_upload)} images, found {s3_image_count} in S3\")\n",
    "\n",
    "if s3_label_count != len(annotations_to_upload):\n",
    "    print(f\"\\n\u26a0\ufe0f  Warning: Annotation count mismatch!\")\n",
    "    print(f\"   Expected {len(annotations_to_upload)} annotations, found {s3_label_count} in S3\")\n",
    "\n",
    "if not data_yaml_exists:\n",
    "    print(f\"\\n\u274c Error: data.yaml is missing from S3!\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"data.yaml not found in S3.\\n\"\n",
    "        f\"Expected location: s3://{default_bucket}/{data_yaml_s3_key}\\n\"\n",
    "        f\"This file is required for YOLO training.\"\n",
    "    )\n",
    "\n",
    "if (s3_image_count == len(images_to_upload) and \n",
    "    s3_label_count == len(annotations_to_upload) and \n",
    "    data_yaml_exists):\n",
    "    print(\"\\n\u2705 All files uploaded successfully! Dataset ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Train YOLOv8 models using SageMaker training jobs with custom PyTorch training scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create PyTorch Estimator\n",
    "\n",
    "Configure the PyTorch estimator with the custom training script and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Training script configuration\n",
    "training_script = 'yolo_training.py'\n",
    "\n",
    "# Verify training script exists\n",
    "if not os.path.exists(training_script):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Training script not found: {training_script}\\n\"\n",
    "        f\"Expected location: {os.path.abspath(training_script)}\\n\"\n",
    "        f\"Please ensure the training script is in the current directory.\"\n",
    "    )\n",
    "\n",
    "# Hyperparameters for YOLO training\n",
    "hyperparameters = {\n",
    "    'model-size': 'yolov8n',  # Options: yolov8n, yolov8s, yolov8m, yolov8l, yolov8x\n",
    "    'task': 'detect',          # Options: detect, segment\n",
    "    'epochs': 50,              # Number of training epochs\n",
    "    'batch-size': 16,          # Training batch size\n",
    "    'img-size': 640,           # Input image size\n",
    "}\n",
    "\n",
    "# Create PyTorch estimator\n",
    "print(\"\ud83d\ude80 Creating PyTorch estimator...\\n\")\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point=training_script,\n",
    "    role=role,\n",
    "    framework_version='2.0.0',\n",
    "    py_version='py310',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.4xlarge',  # GPU instance for training\n",
    "    volume_size=20,  # GB\n",
    "    max_run=7200,  # Maximum runtime in seconds (2 hours)\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_paths['training_output'],\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='yolo-cookie-training'\n",
    ")\n",
    "\n",
    "print(\"\u2705 PyTorch estimator created successfully\")\n",
    "print(\"\\n\ud83d\udcca Estimator Configuration:\")\n",
    "print(f\"   Framework: PyTorch {pytorch_estimator.framework_version}\")\n",
    "print(f\"   Python version: {pytorch_estimator.py_version}\")\n",
    "print(f\"   Instance type: {pytorch_estimator.instance_type}\")\n",
    "print(f\"   Instance count: {pytorch_estimator.instance_count}\")\n",
    "print(f\"   Volume size: {pytorch_estimator.volume_size} GB\")\n",
    "print(f\"   Max runtime: {pytorch_estimator.max_run} seconds ({pytorch_estimator.max_run // 3600} hours)\")\n",
    "print(f\"   Output path: {pytorch_estimator.output_path}\")\n",
    "\n",
    "print(\"\\n\ud83c\udfaf Hyperparameters:\")\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Configure Input Data Channels\n",
    "\n",
    "Specify the S3 URI for the training data and configure the data distribution type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Define S3 input data path\n",
    "training_data_s3_uri = f\"s3://{default_bucket}/{s3_dataset_prefix}/\"\n",
    "\n",
    "# Create training input configuration\n",
    "training_input = TrainingInput(\n",
    "    s3_data=training_data_s3_uri,\n",
    "    distribution='FullyReplicated',  # Copy all data to each instance\n",
    "    content_type='application/x-directory',\n",
    "    s3_data_type='S3Prefix'\n",
    ")\n",
    "\n",
    "# Create input channels dictionary\n",
    "input_channels = {\n",
    "    'training': training_input\n",
    "}\n",
    "\n",
    "print(\"\u2705 Input data channels configured\")\n",
    "print(\"\\n\ud83d\udccd Training Data Configuration:\")\n",
    "print(f\"   S3 URI: {training_data_s3_uri}\")\n",
    "print(f\"   Distribution: FullyReplicated\")\n",
    "print(f\"   Content type: application/x-directory\")\n",
    "print(f\"   S3 data type: S3Prefix\")\n",
    "\n",
    "# Verify S3 data exists\n",
    "print(\"\\n\ud83d\udd0d Verifying training data in S3...\")\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=default_bucket,\n",
    "        Prefix=s3_dataset_prefix,\n",
    "        MaxKeys=10\n",
    "    )\n",
    "    \n",
    "    if response.get('KeyCount', 0) > 0:\n",
    "        print(f\"\u2705 Training data found in S3\")\n",
    "        print(f\"   Files found: {response.get('KeyCount', 0)}+ objects\")\n",
    "    else:\n",
    "        print(f\"\u274c No training data found in S3\")\n",
    "        print(f\"   Location: {training_data_s3_uri}\")\n",
    "        raise FileNotFoundError(\n",
    "            f\"No training data found in S3.\\n\"\n",
    "            f\"Expected location: {training_data_s3_uri}\\n\"\n",
    "            f\"Please ensure the dataset has been uploaded to S3.\"\n",
    "        )\n",
    "        \n",
    "except ClientError as e:\n",
    "    print(f\"\u274c Failed to verify training data: {e.response['Error']['Code']}\")\n",
    "    print(f\"   Bucket: {default_bucket}\")\n",
    "    print(f\"   Prefix: {s3_dataset_prefix}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Launch Training Job\n",
    "\n",
    "Generate a unique training job name and launch the SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique training job name with timestamp\n",
    "training_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "training_job_name = f\"yolo-cookie-{hyperparameters['task']}-{training_timestamp}\"\n",
    "\n",
    "print(f\"\ud83d\ude80 Launching training job: {training_job_name}\\n\")\n",
    "\n",
    "# Launch training job\n",
    "try:\n",
    "    pytorch_estimator.fit(\n",
    "        inputs=input_channels,\n",
    "        job_name=training_job_name,\n",
    "        wait=False  # Don't wait for completion in this cell\n",
    "    )\n",
    "    \n",
    "    # Get training job details\n",
    "    training_job_arn = pytorch_estimator.latest_training_job.describe()['TrainingJobArn']\n",
    "    training_job_status = pytorch_estimator.latest_training_job.describe()['TrainingJobStatus']\n",
    "    \n",
    "    print(\"\u2705 Training job launched successfully!\\n\")\n",
    "    print(\"\ud83d\udcca Training Job Details:\")\n",
    "    print(f\"   Job name: {training_job_name}\")\n",
    "    print(f\"   Job ARN: {training_job_arn}\")\n",
    "    print(f\"   Status: {training_job_status}\")\n",
    "    print(f\"   Model: {hyperparameters['model-size']}\")\n",
    "    print(f\"   Task: {hyperparameters['task']}\")\n",
    "    print(f\"   Epochs: {hyperparameters['epochs']}\")\n",
    "    print(f\"   Batch size: {hyperparameters['batch-size']}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd17 View training job in SageMaker Console:\")\n",
    "    console_url = f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/jobs/{training_job_name}\"\n",
    "    print(f\"   {console_url}\")\n",
    "    \n",
    "    print(f\"\\n\u2139\ufe0f  Training job is running. Use the next cell to monitor progress.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Failed to launch training job: {str(e)}\")\n",
    "    print(f\"\\n\ud83d\udd0d Troubleshooting:\")\n",
    "    print(f\"   - Verify the training script exists: {training_script}\")\n",
    "    print(f\"   - Verify training data is in S3: {training_data_s3_uri}\")\n",
    "    print(f\"   - Check IAM role permissions: {role}\")\n",
    "    print(f\"   - Verify instance type availability: {pytorch_estimator.instance_type}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Monitor Training Job\n",
    "\n",
    "Poll the training job status and display progress until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create SageMaker client for monitoring\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(f\"\\ud83d\\udd0d Monitoring training job: {training_job_name}\\n\")\n",
    "print(\"Status updates every 60 seconds...\\n\")\n",
    "\n",
    "# Initialize monitoring variables\n",
    "previous_status = None\n",
    "start_time = time.time()\n",
    "dots_printed = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get training job status\n",
    "        response = sagemaker_client.describe_training_job(\n",
    "            TrainingJobName=training_job_name\n",
    "        )\n",
    "        \n",
    "        current_status = response['TrainingJobStatus']\n",
    "        \n",
    "        # Display status change\n",
    "        if current_status != previous_status:\n",
    "            if previous_status is not None:\n",
    "                print()  # New line after dots\n",
    "            \n",
    "            elapsed_time = int(time.time() - start_time)\n",
    "            elapsed_minutes = elapsed_time // 60\n",
    "            elapsed_seconds = elapsed_time % 60\n",
    "            \n",
    "            print(f\"[{elapsed_minutes:02d}:{elapsed_seconds:02d}] Status: {current_status}\")\n",
    "            previous_status = current_status\n",
    "            dots_printed = 0\n",
    "        \n",
    "        # Check if training is complete\n",
    "        if current_status in ['Completed', 'Failed', 'Stopped']:\n",
    "            print()  # New line after dots\n",
    "            \n",
    "            # Display completion details\n",
    "            if current_status == 'Completed':\n",
    "                print(\"\\n\\u2705 Training job completed successfully!\\n\")\n",
    "                \n",
    "                # Get model artifact location\n",
    "                model_artifact_s3_uri = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "                \n",
    "                # Display training metrics if available\n",
    "                if 'FinalMetricDataList' in response:\n",
    "                    print(\"\ud83d\udcca Final Training Metrics:\")\n",
    "                    for metric in response['FinalMetricDataList']:\n",
    "                        print(f\"   {metric['MetricName']}: {metric['Value']:.4f}\")\n",
    "                    print()\n",
    "                \n",
    "                # Display training time\n",
    "                training_time_seconds = response.get('TrainingTimeInSeconds', 0)\n",
    "                training_hours = training_time_seconds // 3600\n",
    "                training_minutes = (training_time_seconds % 3600) // 60\n",
    "                training_seconds = training_time_seconds % 60\n",
    "                \n",
    "                print(\"\u23f1\ufe0f  Training Duration:\")\n",
    "                if training_hours > 0:\n",
    "                    print(f\"   {training_hours}h {training_minutes}m {training_seconds}s\")\n",
    "                else:\n",
    "                    print(f\"   {training_minutes}m {training_seconds}s\")\n",
    "                \n",
    "                # Display billable time\n",
    "                billable_time_seconds = response.get('BillableTimeInSeconds', 0)\n",
    "                billable_minutes = billable_time_seconds // 60\n",
    "                billable_seconds = billable_time_seconds % 60\n",
    "                print(f\"   Billable time: {billable_minutes}m {billable_seconds}s\")\n",
    "                \n",
    "                # Display model artifact location\n",
    "                print(f\"\\n\ud83d\udce6 Trained Model Artifact:\")\n",
    "                print(f\"   S3 URI: {model_artifact_s3_uri}\")\n",
    "                \n",
    "                # Store for later use\n",
    "                trained_model_s3_uri = model_artifact_s3_uri\n",
    "                \n",
    "            elif current_status == 'Failed':\n",
    "                print(\"\\n\\u274c Training job failed!\\n\")\n",
    "                \n",
    "                # Get failure reason\n",
    "                failure_reason = response.get('FailureReason', 'Unknown failure reason')\n",
    "                print(f\"\ud83d\udeab Failure Reason:\")\n",
    "                print(f\"   {failure_reason}\\n\")\n",
    "                \n",
    "                # Provide troubleshooting guidance\n",
    "                print(\"\ud83d\udd0d Common Failure Causes:\")\n",
    "                print(\"   1. Insufficient instance resources\")\n",
    "                print(\"      - Try reducing batch size or image size\")\n",
    "                print(\"      - Use a larger instance type\")\n",
    "                print(\"   2. Invalid data format\")\n",
    "                print(\"      - Verify YOLO annotation format is correct\")\n",
    "                print(\"      - Check data.yaml configuration\")\n",
    "                print(\"   3. Training timeout exceeded\")\n",
    "                print(\"      - Reduce number of epochs\")\n",
    "                print(\"      - Increase max_run parameter\")\n",
    "                print(\"   4. Missing dependencies\")\n",
    "                print(\"      - Verify training script has all required imports\")\n",
    "                print(\"      - Check PyTorch and Ultralytics versions\")\n",
    "                \n",
    "                # Display CloudWatch logs link\n",
    "                print(f\"\\n\ud83d\udd17 View detailed logs in CloudWatch:\")\n",
    "                log_group = f\"/aws/sagemaker/TrainingJobs\"\n",
    "                log_stream = training_job_name\n",
    "                logs_url = (\n",
    "                    f\"https://console.aws.amazon.com/cloudwatch/home?region={region}\"\n",
    "                    f\"#logsV2:log-groups/log-group/{log_group.replace('/', '$252F')}\"\n",
    "                    f\"/log-events/{log_stream.replace('/', '$252F')}\"\n",
    "                )\n",
    "                print(f\"   {logs_url}\")\n",
    "                \n",
    "            elif current_status == 'Stopped':\n",
    "                print(\"\\n\u26a0\ufe0f  Training job was stopped.\\n\")\n",
    "                print(\"The training job was manually stopped before completion.\")\n",
    "            \n",
    "            # Exit monitoring loop\n",
    "            break\n",
    "        \n",
    "        # Display progress indicator (dots) for in-progress status\n",
    "        if current_status == 'InProgress':\n",
    "            print('.', end='', flush=True)\n",
    "            dots_printed += 1\n",
    "            \n",
    "            # New line every 60 dots (60 minutes)\n",
    "            if dots_printed >= 60:\n",
    "                print()\n",
    "                dots_printed = 0\n",
    "        \n",
    "        # Wait 60 seconds before next poll\n",
    "        time.sleep(60)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_msg = e.response['Error']['Message']\n",
    "        print(f\"\\n\\u274c Error monitoring training job: {error_code}\")\n",
    "        print(f\"   Job name: {training_job_name}\")\n",
    "        print(f\"   Error: {error_msg}\")\n",
    "        \n",
    "        if error_code == 'ValidationException':\n",
    "            print(f\"\\n\ud83d\udd0d The training job may not exist or the name is incorrect.\")\n",
    "            print(f\"   Verify job name: {training_job_name}\")\n",
    "        \n",
    "        raise\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n\u26a0\ufe0f  Monitoring interrupted by user.\")\n",
    "        print(f\"   Training job is still running: {training_job_name}\")\n",
    "        print(f\"   You can resume monitoring by re-running this cell.\")\n",
    "        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\u274c Unexpected error: {str(e)}\")\n",
    "        print(f\"   Training job: {training_job_name}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Preparation for Compilation\n",
    "\n",
    "Prepare the trained YOLO model for SageMaker Neo compilation by downloading, extracting, and repackaging the model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Download Trained Model from S3\n",
    "\n",
    "Download the model.tar.gz file from S3 to the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Define local directories for model preparation\n",
    "model_prep_dir = 'model-preparation'\n",
    "downloaded_model_dir = os.path.join(model_prep_dir, 'downloaded')\n",
    "extracted_model_dir = os.path.join(model_prep_dir, 'extracted')\n",
    "compiled_model_dir = os.path.join(model_prep_dir, 'for-compilation')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(downloaded_model_dir, exist_ok=True)\n",
    "os.makedirs(extracted_model_dir, exist_ok=True)\n",
    "os.makedirs(compiled_model_dir, exist_ok=True)\n",
    "\n",
    "print(\"\ud83d\udce5 Downloading trained model from S3...\\n\")\n",
    "\n",
    "# Check if trained_model_s3_uri is defined (from training monitoring cell)\n",
    "if 'trained_model_s3_uri' not in locals():\n",
    "    print(\"\u26a0\ufe0f  Warning: trained_model_s3_uri not found.\")\n",
    "    print(\"   This variable should be set by the training monitoring cell.\")\n",
    "    print(\"   Please provide the S3 URI of your trained model:\\n\")\n",
    "    \n",
    "    # Example format for user to fill in\n",
    "    trained_model_s3_uri = input(\"Enter S3 URI (e.g., s3://bucket/path/model.tar.gz): \")\n",
    "    \n",
    "    if not trained_model_s3_uri or not trained_model_s3_uri.startswith('s3://'):\n",
    "        raise ValueError(\n",
    "            f\"Invalid S3 URI provided: {trained_model_s3_uri}\\n\"\n",
    "            f\"Expected format: s3://bucket-name/path/to/model.tar.gz\"\n",
    "        )\n",
    "\n",
    "# Parse S3 URI\n",
    "s3_uri_parts = trained_model_s3_uri.replace('s3://', '').split('/', 1)\n",
    "model_bucket = s3_uri_parts[0]\n",
    "model_key = s3_uri_parts[1]\n",
    "\n",
    "# Local path for downloaded model\n",
    "local_model_path = os.path.join(downloaded_model_dir, 'model.tar.gz')\n",
    "\n",
    "print(f\"\ud83d\udccd Source:\")\n",
    "print(f\"   S3 URI: {trained_model_s3_uri}\")\n",
    "print(f\"   Bucket: {model_bucket}\")\n",
    "print(f\"   Key: {model_key}\")\n",
    "print(f\"\\n\ud83d\udccd Destination:\")\n",
    "print(f\"   Local path: {local_model_path}\\n\")\n",
    "\n",
    "# Download model from S3\n",
    "try:\n",
    "    s3_client.download_file(\n",
    "        Bucket=model_bucket,\n",
    "        Key=model_key,\n",
    "        Filename=local_model_path\n",
    "    )\n",
    "    \n",
    "    # Verify download\n",
    "    if os.path.exists(local_model_path):\n",
    "        file_size_mb = os.path.getsize(local_model_path) / (1024 * 1024)\n",
    "        print(f\"\u2705 Model downloaded successfully\")\n",
    "        print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"   Location: {local_model_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Model file not found after download.\\n\"\n",
    "            f\"Expected location: {local_model_path}\"\n",
    "        )\n",
    "        \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    error_msg = e.response['Error']['Message']\n",
    "    print(f\"\u274c S3 download failed: {error_code}\")\n",
    "    print(f\"   Bucket: {model_bucket}\")\n",
    "    print(f\"   Key: {model_key}\")\n",
    "    print(f\"   Error: {error_msg}\")\n",
    "    \n",
    "    if error_code == 'NoSuchKey':\n",
    "        print(f\"\\n\ud83d\udd0d The model file does not exist at the specified S3 location.\")\n",
    "        print(f\"   Verify the training job completed successfully.\")\n",
    "        print(f\"   Check the S3 URI: {trained_model_s3_uri}\")\n",
    "    elif error_code == 'NoSuchBucket':\n",
    "        print(f\"\\n\ud83d\udd0d The S3 bucket does not exist.\")\n",
    "        print(f\"   Verify the bucket name: {model_bucket}\")\n",
    "    \n",
    "    raise Exception(\n",
    "        f\"Failed to download model from S3.\\n\"\n",
    "        f\"S3 URI: {trained_model_s3_uri}\\n\"\n",
    "        f\"Error: {error_code} - {error_msg}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Unexpected error during download: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Extract and Prepare Model for Compilation\n",
    "\n",
    "Extract the tar.gz file, locate the YOLO model weights, read metadata, and create a compilation-ready package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\udce6 Extracting model archive...\\n\")\n",
    "\n",
    "# Extract tar.gz file\n",
    "try:\n",
    "    with tarfile.open(local_model_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extracted_model_dir)\n",
    "    \n",
    "    print(f\"\u2705 Model archive extracted\")\n",
    "    print(f\"   Location: {extracted_model_dir}\\n\")\n",
    "    \n",
    "except tarfile.TarError as e:\n",
    "    print(f\"\u274c Failed to extract tar.gz file: {str(e)}\")\n",
    "    print(f\"   File: {local_model_path}\")\n",
    "    raise Exception(\n",
    "        f\"Failed to extract model archive.\\n\"\n",
    "        f\"File: {local_model_path}\\n\"\n",
    "        f\"Error: {str(e)}\\n\"\n",
    "        f\"The file may be corrupted or not a valid tar.gz archive.\"\n",
    "    )\n",
    "\n",
    "# List extracted files\n",
    "print(\"\ud83d\udcc1 Extracted files:\")\n",
    "extracted_files = []\n",
    "for root, dirs, files in os.walk(extracted_model_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        rel_path = os.path.relpath(file_path, extracted_model_dir)\n",
    "        file_size_kb = os.path.getsize(file_path) / 1024\n",
    "        extracted_files.append(rel_path)\n",
    "        print(f\"   {rel_path} ({file_size_kb:.1f} KB)\")\n",
    "\n",
    "if not extracted_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No files found in extracted archive.\\n\"\n",
    "        f\"Extraction directory: {extracted_model_dir}\\n\"\n",
    "        f\"The archive may be empty or extraction failed.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Locating YOLO model weights...\\n\")\n",
    "\n",
    "# Look for YOLO model files (best.pt, yolo.pt, or last.pt)\n",
    "model_candidates = ['best.pt', 'yolo.pt', 'last.pt']\n",
    "model_file_path = None\n",
    "\n",
    "for root, dirs, files in os.walk(extracted_model_dir):\n",
    "    for candidate in model_candidates:\n",
    "        if candidate in files:\n",
    "            model_file_path = os.path.join(root, candidate)\n",
    "            print(f\"\u2705 Found model weights: {candidate}\")\n",
    "            print(f\"   Path: {model_file_path}\")\n",
    "            break\n",
    "    if model_file_path:\n",
    "        break\n",
    "\n",
    "if not model_file_path:\n",
    "    print(f\"\u274c Model weights file not found\")\n",
    "    print(f\"   Searched for: {', '.join(model_candidates)}\")\n",
    "    print(f\"   In directory: {extracted_model_dir}\")\n",
    "    print(f\"   Available files: {', '.join(extracted_files)}\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"YOLO model weights not found in extracted archive.\\n\"\n",
    "        f\"Expected files: {', '.join(model_candidates)}\\n\"\n",
    "        f\"Found files: {', '.join(extracted_files)}\\n\"\n",
    "        f\"The training output may not contain the expected model files.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d Reading model metadata...\\n\")\n",
    "\n",
    "# Read model metadata to extract input shape\n",
    "try:\n",
    "    # Load PyTorch model\n",
    "    model_data = torch.load(model_file_path, map_location='cpu')\n",
    "    \n",
    "    # Extract input shape from model metadata\n",
    "    # YOLOv8 models typically store this in the model dict\n",
    "    input_shape = None\n",
    "    \n",
    "    # Try to get input shape from various possible locations\n",
    "    if isinstance(model_data, dict):\n",
    "        # Check for metadata file\n",
    "        metadata_path = os.path.join(extracted_model_dir, 'metadata.json')\n",
    "        if os.path.exists(metadata_path):\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                input_shape = metadata.get('input_shape')\n",
    "                print(f\"\u2705 Found input shape in metadata.json: {input_shape}\")\n",
    "        \n",
    "        # If not in metadata file, try to infer from model\n",
    "        if not input_shape:\n",
    "            # Default YOLO input shape (can be overridden by hyperparameters)\n",
    "            img_size = hyperparameters.get('img-size', 640)\n",
    "            input_shape = [1, 3, img_size, img_size]\n",
    "            print(f\"\u2139\ufe0f  Using default YOLO input shape: {input_shape}\")\n",
    "            print(f\"   (Based on img-size hyperparameter: {img_size})\")\n",
    "    \n",
    "    if not input_shape:\n",
    "        # Fallback to standard YOLO input\n",
    "        input_shape = [1, 3, 640, 640]\n",
    "        print(f\"\u26a0\ufe0f  Could not determine input shape from model\")\n",
    "        print(f\"   Using standard YOLO default: {input_shape}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Model Information:\")\n",
    "    print(f\"   Input shape: {input_shape}\")\n",
    "    print(f\"   Format: [batch_size, channels, height, width]\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f  Warning: Could not read model metadata: {str(e)}\")\n",
    "    print(f\"   Using default input shape: [1, 3, 640, 640]\")\n",
    "    input_shape = [1, 3, 640, 640]\n",
    "\n",
    "print(f\"\\n\ud83d\udce6 Creating compilation-ready package...\\n\")\n",
    "\n",
    "# Create new tar.gz with only the model weights file\n",
    "compilation_model_path = os.path.join(compiled_model_dir, 'model.tar.gz')\n",
    "\n",
    "try:\n",
    "    with tarfile.open(compilation_model_path, 'w:gz') as tar:\n",
    "        # Add model file with arcname to place it at root of archive\n",
    "        tar.add(model_file_path, arcname=os.path.basename(model_file_path))\n",
    "    \n",
    "    # Verify the new archive\n",
    "    if os.path.exists(compilation_model_path):\n",
    "        file_size_mb = os.path.getsize(compilation_model_path) / (1024 * 1024)\n",
    "        print(f\"\u2705 Compilation-ready package created\")\n",
    "        print(f\"   Location: {compilation_model_path}\")\n",
    "        print(f\"   Size: {file_size_mb:.2f} MB\")\n",
    "        print(f\"   Contents: {os.path.basename(model_file_path)}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Failed to create compilation package.\\n\"\n",
    "            f\"Expected location: {compilation_model_path}\"\n",
    "        )\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\u274c Failed to create compilation package: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n\ud83d\udce4 Uploading prepared model to S3...\\n\")\n",
    "\n",
    "# Upload prepared model to S3\n",
    "compilation_model_s3_key = f\"{s3_prefix}/models/model-for-compilation.tar.gz\"\n",
    "compilation_model_s3_uri = f\"s3://{default_bucket}/{compilation_model_s3_key}\"\n",
    "\n",
    "try:\n",
    "    s3_client.upload_file(\n",
    "        compilation_model_path,\n",
    "        default_bucket,\n",
    "        compilation_model_s3_key\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Prepared model uploaded to S3\")\n",
    "    print(f\"   S3 URI: {compilation_model_s3_uri}\")\n",
    "    \n",
    "    # Create DataInputConfig for compilation\n",
    "    data_input_config = json.dumps({\"input_shape\": input_shape})\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Compilation Configuration:\")\n",
    "    print(f\"   Model S3 URI: {compilation_model_s3_uri}\")\n",
    "    print(f\"   DataInputConfig: {data_input_config}\")\n",
    "    print(f\"   Framework: PYTORCH\")\n",
    "    print(f\"   Framework Version: 2.0\")\n",
    "    \n",
    "    print(f\"\\n\u2705 Model preparation complete! Ready for compilation.\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    error_msg = e.response['Error']['Message']\n",
    "    print(f\"\u274c S3 upload failed: {error_code}\")\n",
    "    print(f\"   File: {compilation_model_path}\")\n",
    "    print(f\"   Destination: {compilation_model_s3_uri}\")\n",
    "    print(f\"   Error: {error_msg}\")\n",
    "    raise Exception(\n",
    "        f\"Failed to upload prepared model to S3.\\n\"\n",
    "        f\"File: {compilation_model_path}\\n\"\n",
    "        f\"S3 URI: {compilation_model_s3_uri}\\n\"\n",
    "        f\"Error: {error_code} - {error_msg}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Compilation\n",
    "\n",
    "Compile trained models for target platforms using SageMaker Neo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Compile Model for Jetson Xavier GPU\n",
    "\n",
    "Create and submit a compilation job for Jetson Xavier hardware with GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required variables are available\n",
    "if 'compilation_model_s3_uri' not in locals():\n",
    "    print(\"\u26a0\ufe0f  Warning: compilation_model_s3_uri not found.\")\n",
    "    print(\"   This variable should be set by the model preparation cells.\")\n",
    "    print(\"   Please provide the S3 URI of your prepared model:\\n\")\n",
    "    \n",
    "    compilation_model_s3_uri = input(\"Enter S3 URI (e.g., s3://bucket/path/model-for-compilation.tar.gz): \")\n",
    "    \n",
    "    if not compilation_model_s3_uri or not compilation_model_s3_uri.startswith('s3://'):\n",
    "        raise ValueError(\n",
    "            f\"Invalid S3 URI provided: {compilation_model_s3_uri}\\n\"\n",
    "            f\"Expected format: s3://bucket-name/path/to/model-for-compilation.tar.gz\"\n",
    "        )\n",
    "\n",
    "if 'data_input_config' not in locals():\n",
    "    print(\"\u26a0\ufe0f  Warning: data_input_config not found.\")\n",
    "    print(\"   Using default YOLO input shape: [1, 3, 640, 640]\\n\")\n",
    "    data_input_config = json.dumps({\"input_shape\": [1, 3, 640, 640]})\n",
    "\n",
    "print(\"\ud83d\ude80 Creating compilation job for Jetson Xavier GPU...\\n\")\n",
    "\n",
    "# Generate unique compilation job name\n",
    "compilation_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "compilation_job_name_xavier = f\"yolo-cookie-xavier-{compilation_timestamp}\"\n",
    "\n",
    "# Define target platform for Jetson Xavier\n",
    "target_platform_xavier = {\n",
    "    'Os': 'LINUX',\n",
    "    'Arch': 'ARM64',\n",
    "    'Accelerator': 'NVIDIA'\n",
    "}\n",
    "\n",
    "# Compiler options for Jetson Xavier\n",
    "# Xavier uses CUDA 10.2, GPU code sm_72, TensorRT 8.2.1, FP16 precision\n",
    "compiler_options_xavier = json.dumps({\n",
    "    'cuda-ver': '10.2',\n",
    "    'gpu-code': 'sm_72',\n",
    "    'trt-ver': '8.2.1',\n",
    "    'precision_mode': 'fp16',\n",
    "    'jetson-platform': 'xavier'\n",
    "})\n",
    "\n",
    "# Output S3 location for compiled model\n",
    "compilation_output_s3_xavier = f\"{s3_paths['compilation_output']}/jetson-xavier/\"\n",
    "\n",
    "print(\"\ud83d\udcca Compilation Job Configuration:\")\n",
    "print(f\"   Job name: {compilation_job_name_xavier}\")\n",
    "print(f\"   Input model: {compilation_model_s3_uri}\")\n",
    "print(f\"   Framework: PYTORCH 2.0\")\n",
    "print(f\"   DataInputConfig: {data_input_config}\")\n",
    "print(f\"\\n\ud83c\udfaf Target Platform:\")\n",
    "print(f\"   OS: {target_platform_xavier['Os']}\")\n",
    "print(f\"   Architecture: {target_platform_xavier['Arch']}\")\n",
    "print(f\"   Accelerator: {target_platform_xavier['Accelerator']}\")\n",
    "print(f\"\\n\u2699\ufe0f  Compiler Options:\")\n",
    "compiler_opts = json.loads(compiler_options_xavier)\n",
    "for key, value in compiler_opts.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(f\"\\n\ud83d\udccd Output Location:\")\n",
    "print(f\"   S3 URI: {compilation_output_s3_xavier}\")\n",
    "\n",
    "# Create compilation job\n",
    "print(f\"\\n\ud83d\ude80 Submitting compilation job...\\n\")\n",
    "\n",
    "try:\n",
    "    response_xavier = sagemaker_client.create_compilation_job(\n",
    "        CompilationJobName=compilation_job_name_xavier,\n",
    "        RoleArn=role,\n",
    "        InputConfig={\n",
    "            'S3Uri': compilation_model_s3_uri,\n",
    "            'DataInputConfig': data_input_config,\n",
    "            'Framework': 'PYTORCH',\n",
    "            'FrameworkVersion': '2.0'\n",
    "        },\n",
    "        OutputConfig={\n",
    "            'S3OutputLocation': compilation_output_s3_xavier,\n",
    "            'TargetPlatform': target_platform_xavier,\n",
    "            'CompilerOptions': compiler_options_xavier\n",
    "        },\n",
    "        StoppingCondition={\n",
    "            'MaxRuntimeInSeconds': 900  # 15 minutes\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get compilation job ARN\n",
    "    compilation_job_arn_xavier = response_xavier['CompilationJobArn']\n",
    "    \n",
    "    print(\"\u2705 Compilation job submitted successfully!\\n\")\n",
    "    print(\"\ud83d\udcca Compilation Job Details:\")\n",
    "    print(f\"   Job name: {compilation_job_name_xavier}\")\n",
    "    print(f\"   Job ARN: {compilation_job_arn_xavier}\")\n",
    "    print(f\"   Target: Jetson Xavier GPU\")\n",
    "    print(f\"   Status: STARTING\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd17 View compilation job in SageMaker Console:\")\n",
    "    console_url = f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/compilation-jobs/{compilation_job_name_xavier}\"\n",
    "    print(f\"   {console_url}\")\n",
    "    \n",
    "    print(f\"\\n\u2139\ufe0f  Compilation job is running. Use the monitoring cell to track progress.\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    error_msg = e.response['Error']['Message']\n",
    "    print(f\"\u274c Failed to create compilation job: {error_code}\")\n",
    "    print(f\"   Job name: {compilation_job_name_xavier}\")\n",
    "    print(f\"   Error: {error_msg}\\n\")\n",
    "    \n",
    "    print(\"\ud83d\udd0d Troubleshooting:\")\n",
    "    print(\"   1. Verify the prepared model exists in S3\")\n",
    "    print(f\"      Model URI: {compilation_model_s3_uri}\")\n",
    "    print(\"   2. Check IAM role permissions for SageMaker Neo\")\n",
    "    print(f\"      Role: {role}\")\n",
    "    print(\"   3. Verify DataInputConfig format\")\n",
    "    print(f\"      Config: {data_input_config}\")\n",
    "    print(\"   4. Check compiler options compatibility\")\n",
    "    print(f\"      Options: {compiler_options_xavier}\")\n",
    "    \n",
    "    if error_code == 'ValidationException':\n",
    "        print(\"\\n\u26a0\ufe0f  Validation Error: Check input parameters\")\n",
    "        print(\"   - Ensure model S3 URI is valid\")\n",
    "        print(\"   - Verify DataInputConfig JSON format\")\n",
    "        print(\"   - Check target platform configuration\")\n",
    "    \n",
    "    raise Exception(\n",
    "        f\"Failed to create compilation job for Jetson Xavier.\\n\"\n",
    "        f\"Job name: {compilation_job_name_xavier}\\n\"\n",
    "        f\"Error: {error_code} - {error_msg}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Unexpected error: {str(e)}\")\n",
    "    print(f\"   Job name: {compilation_job_name_xavier}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Compile Model for x86_64 CPU\n",
    "\n",
    "Create and submit a compilation job for x86_64 CPU hardware (standard server infrastructure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required variables are available\n",
    "if 'compilation_model_s3_uri' not in locals():\n",
    "    print(\"\u26a0\ufe0f  Warning: compilation_model_s3_uri not found.\")\n",
    "    print(\"   This variable should be set by the model preparation cells.\")\n",
    "    print(\"   Please provide the S3 URI of your prepared model:\\n\")\n",
    "    \n",
    "    compilation_model_s3_uri = input(\"Enter S3 URI (e.g., s3://bucket/path/model-for-compilation.tar.gz): \")\n",
    "    \n",
    "    if not compilation_model_s3_uri or not compilation_model_s3_uri.startswith('s3://'):\n",
    "        raise ValueError(\n",
    "            f\"Invalid S3 URI provided: {compilation_model_s3_uri}\\n\"\n",
    "            f\"Expected format: s3://bucket-name/path/to/model-for-compilation.tar.gz\"\n",
    "        )\n",
    "\n",
    "if 'data_input_config' not in locals():\n",
    "    print(\"\u26a0\ufe0f  Warning: data_input_config not found.\")\n",
    "    print(\"   Using default YOLO input shape: [1, 3, 640, 640]\\n\")\n",
    "    data_input_config = json.dumps({\"input_shape\": [1, 3, 640, 640]})\n",
    "\n",
    "print(\"\ud83d\ude80 Creating compilation job for x86_64 CPU...\\n\")\n",
    "\n",
    "# Generate unique compilation job name\n",
    "compilation_timestamp_x86 = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "compilation_job_name_x86 = f\"yolo-cookie-x86-cpu-{compilation_timestamp_x86}\"\n",
    "\n",
    "# Define target platform for x86_64 CPU\n",
    "target_platform_x86 = {\n",
    "    'Os': 'LINUX',\n",
    "    'Arch': 'X86_64'\n",
    "}\n",
    "\n",
    "# Output S3 location for compiled model\n",
    "compilation_output_s3_x86 = f\"{s3_paths['compilation_output']}/x86-64-cpu/\"\n",
    "\n",
    "print(\"\ud83d\udcca Compilation Job Configuration:\")\n",
    "print(f\"   Job name: {compilation_job_name_x86}\")\n",
    "print(f\"   Input model: {compilation_model_s3_uri}\")\n",
    "print(f\"   Framework: PYTORCH 2.0\")\n",
    "print(f\"   DataInputConfig: {data_input_config}\")\n",
    "print(f\"\\n\ud83c\udfaf Target Platform:\")\n",
    "print(f\"   OS: {target_platform_x86['Os']}\")\n",
    "print(f\"   Architecture: {target_platform_x86['Arch']}\")\n",
    "print(f\"   Accelerator: None (CPU only)\")\n",
    "print(f\"\\n\ud83d\udccd Output Location:\")\n",
    "print(f\"   S3 URI: {compilation_output_s3_x86}\")\n",
    "\n",
    "# Create compilation job\n",
    "print(f\"\\n\ud83d\ude80 Submitting compilation job...\\n\")\n",
    "\n",
    "try:\n",
    "    response_x86 = sagemaker_client.create_compilation_job(\n",
    "        CompilationJobName=compilation_job_name_x86,\n",
    "        RoleArn=role,\n",
    "        InputConfig={\n",
    "            'S3Uri': compilation_model_s3_uri,\n",
    "            'DataInputConfig': data_input_config,\n",
    "            'Framework': 'PYTORCH',\n",
    "            'FrameworkVersion': '2.0'\n",
    "        },\n",
    "        OutputConfig={\n",
    "            'S3OutputLocation': compilation_output_s3_x86,\n",
    "            'TargetPlatform': target_platform_x86\n",
    "        },\n",
    "        StoppingCondition={\n",
    "            'MaxRuntimeInSeconds': 900  # 15 minutes\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get compilation job ARN\n",
    "    compilation_job_arn_x86 = response_x86['CompilationJobArn']\n",
    "    \n",
    "    print(\"\u2705 Compilation job submitted successfully!\\n\")\n",
    "    print(\"\ud83d\udcca Compilation Job Details:\")\n",
    "    print(f\"   Job name: {compilation_job_name_x86}\")\n",
    "    print(f\"   Job ARN: {compilation_job_arn_x86}\")\n",
    "    print(f\"   Target: x86_64 CPU\")\n",
    "    print(f\"   Status: STARTING\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd17 View compilation job in SageMaker Console:\")\n",
    "    console_url = f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/compilation-jobs/{compilation_job_name_x86}\"\n",
    "    print(f\"   {console_url}\")\n",
    "    \n",
    "    print(f\"\\n\u2139\ufe0f  Compilation job is running. Use the monitoring cell to track progress.\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    error_msg = e.response['Error']['Message']\n",
    "    print(f\"\u274c Failed to create compilation job: {error_code}\")\n",
    "    print(f\"   Job name: {compilation_job_name_x86}\")\n",
    "    print(f\"   Error: {error_msg}\\n\")\n",
    "    \n",
    "    print(\"\ud83d\udd0d Troubleshooting:\")\n",
    "    print(\"   1. Verify the prepared model exists in S3\")\n",
    "    print(f\"      Model URI: {compilation_model_s3_uri}\")\n",
    "    print(\"   2. Check IAM role permissions for SageMaker Neo\")\n",
    "    print(f\"      Role: {role}\")\n",
    "    print(\"   3. Verify DataInputConfig format\")\n",
    "    print(f\"      Config: {data_input_config}\")\n",
    "    print(\"   4. Check target platform configuration\")\n",
    "    print(f\"      Platform: {target_platform_x86}\")\n",
    "    \n",
    "    if error_code == 'ValidationException':\n",
    "        print(\"\\n\u26a0\ufe0f  Validation Error: Check input parameters\")\n",
    "        print(\"   - Ensure model S3 URI is valid\")\n",
    "        print(\"   - Verify DataInputConfig JSON format\")\n",
    "        print(\"   - Check target platform configuration\")\n",
    "    \n",
    "    raise Exception(\n",
    "        f\"Failed to create compilation job for x86_64 CPU.\\n\"\n",
    "        f\"Job name: {compilation_job_name_x86}\\n\"\n",
    "        f\"Error: {error_code} - {error_msg}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Unexpected error: {str(e)}\")\n",
    "    print(f\"   Job name: {compilation_job_name_x86}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Compile Model for ARM64 CPU\n",
    "\n",
    "Create and submit a compilation job for ARM64 CPU hardware (ARM-based edge devices without GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required variables are available\n",
    "if 'compilation_model_s3_uri' not in locals():\n",
    "    print(\"\u26a0\ufe0f  Warning: compilation_model_s3_uri not found.\")\n",
    "    print(\"   This variable should be set by the model preparation cells.\")\n",
    "    print(\"   Please provide the S3 URI of your prepared model:\\n\")\n",
    "    \n",
    "    compilation_model_s3_uri = input(\"Enter S3 URI (e.g., s3://bucket/path/model-for-compilation.tar.gz): \")\n",
    "    \n",
    "    if not compilation_model_s3_uri or not compilation_model_s3_uri.startswith('s3://'):\n",
    "        raise ValueError(\n",
    "            f\"Invalid S3 URI provided: {compilation_model_s3_uri}\\n\"\n",
    "            f\"Expected format: s3://bucket-name/path/to/model-for-compilation.tar.gz\"\n",
    "        )\n",
    "\n",
    "if 'data_input_config' not in locals():\n",
    "    print(\"\u26a0\ufe0f  Warning: data_input_config not found.\")\n",
    "    print(\"   Using default YOLO input shape: [1, 3, 640, 640]\\n\")\n",
    "    data_input_config = json.dumps({\"input_shape\": [1, 3, 640, 640]})\n",
    "\n",
    "print(\"\ud83d\ude80 Creating compilation job for ARM64 CPU...\\n\")\n",
    "\n",
    "# Generate unique compilation job name\n",
    "compilation_timestamp_arm64 = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "compilation_job_name_arm64 = f\"yolo-cookie-arm64-cpu-{compilation_timestamp_arm64}\"\n",
    "\n",
    "# Define target platform for ARM64 CPU\n",
    "target_platform_arm64 = {\n",
    "    'Os': 'LINUX',\n",
    "    'Arch': 'ARM64'\n",
    "}\n",
    "\n",
    "# Output S3 location for compiled model\n",
    "compilation_output_s3_arm64 = f\"{s3_paths['compilation_output']}/arm64-cpu/\"\n",
    "\n",
    "print(\"\ud83d\udcca Compilation Job Configuration:\")\n",
    "print(f\"   Job name: {compilation_job_name_arm64}\")\n",
    "print(f\"   Input model: {compilation_model_s3_uri}\")\n",
    "print(f\"   Framework: PYTORCH 2.0\")\n",
    "print(f\"   DataInputConfig: {data_input_config}\")\n",
    "print(f\"\\n\ud83c\udfaf Target Platform:\")\n",
    "print(f\"   OS: {target_platform_arm64['Os']}\")\n",
    "print(f\"   Architecture: {target_platform_arm64['Arch']}\")\n",
    "print(f\"   Accelerator: None (CPU only)\")\n",
    "print(f\"\\n\ud83d\udccd Output Location:\")\n",
    "print(f\"   S3 URI: {compilation_output_s3_arm64}\")\n",
    "\n",
    "# Create compilation job\n",
    "print(f\"\\n\ud83d\ude80 Submitting compilation job...\\n\")\n",
    "\n",
    "try:\n",
    "    response_arm64 = sagemaker_client.create_compilation_job(\n",
    "        CompilationJobName=compilation_job_name_arm64,\n",
    "        RoleArn=role,\n",
    "        InputConfig={\n",
    "            'S3Uri': compilation_model_s3_uri,\n",
    "            'DataInputConfig': data_input_config,\n",
    "            'Framework': 'PYTORCH',\n",
    "            'FrameworkVersion': '2.0'\n",
    "        },\n",
    "        OutputConfig={\n",
    "            'S3OutputLocation': compilation_output_s3_arm64,\n",
    "            'TargetPlatform': target_platform_arm64\n",
    "        },\n",
    "        StoppingCondition={\n",
    "            'MaxRuntimeInSeconds': 900  # 15 minutes\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Get compilation job ARN\n",
    "    compilation_job_arn_arm64 = response_arm64['CompilationJobArn']\n",
    "    \n",
    "    print(\"\u2705 Compilation job submitted successfully!\\n\")\n",
    "    print(\"\ud83d\udcca Compilation Job Details:\")\n",
    "    print(f\"   Job name: {compilation_job_name_arm64}\")\n",
    "    print(f\"   Job ARN: {compilation_job_arn_arm64}\")\n",
    "    print(f\"   Target: ARM64 CPU\")\n",
    "    print(f\"   Status: STARTING\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd17 View compilation job in SageMaker Console:\")\n",
    "    console_url = f\"https://console.aws.amazon.com/sagemaker/home?region={region}#/compilation-jobs/{compilation_job_name_arm64}\"\n",
    "    print(f\"   {console_url}\")\n",
    "    \n",
    "    print(f\"\\n\u2139\ufe0f  Compilation job is running. Use the monitoring cell to track progress.\")\n",
    "    \n",
    "except ClientError as e:\n",
    "    error_code = e.response['Error']['Code']\n",
    "    error_msg = e.response['Error']['Message']\n",
    "    print(f\"\u274c Failed to create compilation job: {error_code}\")\n",
    "    print(f\"   Job name: {compilation_job_name_arm64}\")\n",
    "    print(f\"   Error: {error_msg}\\n\")\n",
    "    \n",
    "    print(\"\ud83d\udd0d Troubleshooting:\")\n",
    "    print(\"   1. Verify the prepared model exists in S3\")\n",
    "    print(f\"      Model URI: {compilation_model_s3_uri}\")\n",
    "    print(\"   2. Check IAM role permissions for SageMaker Neo\")\n",
    "    print(f\"      Role: {role}\")\n",
    "    print(\"   3. Verify DataInputConfig format\")\n",
    "    print(f\"      Config: {data_input_config}\")\n",
    "    print(\"   4. Check target platform configuration\")\n",
    "    print(f\"      Platform: {target_platform_arm64}\")\n",
    "    \n",
    "    if error_code == 'ValidationException':\n",
    "        print(\"\\n\u26a0\ufe0f  Validation Error: Check input parameters\")\n",
    "        print(\"   - Ensure model S3 URI is valid\")\n",
    "        print(\"   - Verify DataInputConfig JSON format\")\n",
    "        print(\"   - Check target platform configuration\")\n",
    "    \n",
    "    raise Exception(\n",
    "        f\"Failed to create compilation job for ARM64 CPU.\\n\"\n",
    "        f\"Job name: {compilation_job_name_arm64}\\n\"\n",
    "        f\"Error: {error_code} - {error_msg}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Unexpected error: {str(e)}\")\n",
    "    print(f\"   Job name: {compilation_job_name_arm64}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Monitor Compilation Job\n",
    "\n",
    "Poll the compilation job status and display progress until completion. This cell can monitor any of the compilation jobs created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Determine which compilation job to monitor\n",
    "# Priority: ARM64 > x86_64 > Xavier (monitor the most recently created)\n",
    "compilation_job_to_monitor = None\n",
    "platform_name = None\n",
    "\n",
    "if 'compilation_job_name_arm64' in locals():\n",
    "    compilation_job_to_monitor = compilation_job_name_arm64\n",
    "    platform_name = \"ARM64 CPU\"\n",
    "elif 'compilation_job_name_x86' in locals():\n",
    "    compilation_job_to_monitor = compilation_job_name_x86\n",
    "    platform_name = \"x86_64 CPU\"\n",
    "elif 'compilation_job_name_xavier' in locals():\n",
    "    compilation_job_to_monitor = compilation_job_name_xavier\n",
    "    platform_name = \"Jetson Xavier GPU\"\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No compilation job found to monitor.\")\n",
    "    print(\"   Please run one of the compilation job cells above first.\")\n",
    "    print(\"   Or manually specify a compilation job name:\\n\")\n",
    "    \n",
    "    compilation_job_to_monitor = input(\"Enter compilation job name: \")\n",
    "    platform_name = \"Custom\"\n",
    "    \n",
    "    if not compilation_job_to_monitor:\n",
    "        raise ValueError(\"No compilation job name provided.\")\n",
    "\n",
    "print(f\"\ud83d\udd0d Monitoring compilation job: {compilation_job_to_monitor}\")\n",
    "print(f\"   Target platform: {platform_name}\")\n",
    "print(\"\\nStatus updates every 60 seconds...\\n\")\n",
    "\n",
    "# Initialize monitoring variables\n",
    "previous_status = None\n",
    "start_time = time.time()\n",
    "dots_printed = 0\n",
    "asterisks_printed = 0\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get compilation job status\n",
    "        response = sagemaker_client.describe_compilation_job(\n",
    "            CompilationJobName=compilation_job_to_monitor\n",
    "        )\n",
    "        \n",
    "        current_status = response['CompilationJobStatus']\n",
    "        \n",
    "        # Display status change\n",
    "        if current_status != previous_status:\n",
    "            if previous_status is not None:\n",
    "                print()  # New line after progress indicators\n",
    "            \n",
    "            elapsed_time = int(time.time() - start_time)\n",
    "            elapsed_minutes = elapsed_time // 60\n",
    "            elapsed_seconds = elapsed_time % 60\n",
    "            \n",
    "            print(f\"[{elapsed_minutes:02d}:{elapsed_seconds:02d}] Status: {current_status}\")\n",
    "            previous_status = current_status\n",
    "            dots_printed = 0\n",
    "            asterisks_printed = 0\n",
    "        \n",
    "        # Check if compilation is complete\n",
    "        if current_status in ['COMPLETED', 'FAILED', 'STOPPED']:\n",
    "            print()  # New line after progress indicators\n",
    "            \n",
    "            # Display completion details\n",
    "            if current_status == 'COMPLETED':\n",
    "                print(\"\\n\u2705 Compilation job completed successfully!\\n\")\n",
    "                \n",
    "                # Get compiled model artifact location\n",
    "                compiled_model_s3_uri = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "                \n",
    "                # Display compilation time\n",
    "                compilation_start = response.get('CompilationStartTime')\n",
    "                compilation_end = response.get('CompilationEndTime')\n",
    "                \n",
    "                if compilation_start and compilation_end:\n",
    "                    compilation_duration = (compilation_end - compilation_start).total_seconds()\n",
    "                    compilation_minutes = int(compilation_duration // 60)\n",
    "                    compilation_seconds = int(compilation_duration % 60)\n",
    "                    \n",
    "                    print(\"\u23f1\ufe0f  Compilation Duration:\")\n",
    "                    if compilation_minutes > 0:\n",
    "                        print(f\"   {compilation_minutes}m {compilation_seconds}s\")\n",
    "                    else:\n",
    "                        print(f\"   {compilation_seconds}s\")\n",
    "                    print()\n",
    "                \n",
    "                # Display compiled model artifact location\n",
    "                print(f\"\ud83d\udce6 Compiled Model Artifact:\")\n",
    "                print(f\"   Platform: {platform_name}\")\n",
    "                print(f\"   S3 URI: {compiled_model_s3_uri}\")\n",
    "                \n",
    "                # Display target platform details\n",
    "                if 'OutputConfig' in response:\n",
    "                    output_config = response['OutputConfig']\n",
    "                    if 'TargetPlatform' in output_config:\n",
    "                        target = output_config['TargetPlatform']\n",
    "                        print(f\"\\n\ud83c\udfaf Target Platform Details:\")\n",
    "                        print(f\"   OS: {target.get('Os', 'N/A')}\")\n",
    "                        print(f\"   Architecture: {target.get('Arch', 'N/A')}\")\n",
    "                        if 'Accelerator' in target:\n",
    "                            print(f\"   Accelerator: {target['Accelerator']}\")\n",
    "                \n",
    "                # Store for later use\n",
    "                if platform_name == \"Jetson Xavier GPU\":\n",
    "                    compiled_model_xavier_s3_uri = compiled_model_s3_uri\n",
    "                elif platform_name == \"x86_64 CPU\":\n",
    "                    compiled_model_x86_s3_uri = compiled_model_s3_uri\n",
    "                elif platform_name == \"ARM64 CPU\":\n",
    "                    compiled_model_arm64_s3_uri = compiled_model_s3_uri\n",
    "                \n",
    "            elif current_status == 'FAILED':\n",
    "                print(\"\\n\u274c Compilation job failed!\\n\")\n",
    "                \n",
    "                # Get failure reason\n",
    "                failure_reason = response.get('FailureReason', 'Unknown failure reason')\n",
    "                print(f\"\ud83d\udeab Failure Reason:\")\n",
    "                print(f\"   {failure_reason}\\n\")\n",
    "                \n",
    "                # Provide troubleshooting guidance\n",
    "                print(\"\ud83d\udd0d Common Failure Causes:\")\n",
    "                print(\"   1. Model input shape mismatch\")\n",
    "                print(\"      - Verify DataInputConfig matches model's expected input\")\n",
    "                print(\"      - Check model metadata for correct input shape\")\n",
    "                print(\"   2. Framework version compatibility\")\n",
    "                print(\"      - Ensure PyTorch version is compatible with SageMaker Neo\")\n",
    "                print(\"      - Verify model was trained with supported PyTorch version\")\n",
    "                print(\"   3. Compiler options for target platform\")\n",
    "                print(\"      - Check CUDA version for GPU targets\")\n",
    "                print(\"      - Verify TensorRT version compatibility\")\n",
    "                print(\"      - Ensure compiler options match target hardware\")\n",
    "                print(\"   4. Model architecture not supported\")\n",
    "                print(\"      - Some YOLO operations may not be supported by Neo\")\n",
    "                print(\"      - Check Neo documentation for supported operations\")\n",
    "                \n",
    "                # Display CloudWatch logs link\n",
    "                print(f\"\\n\ud83d\udd17 View detailed logs in CloudWatch:\")\n",
    "                log_group = f\"/aws/sagemaker/CompilationJobs\"\n",
    "                log_stream = compilation_job_to_monitor\n",
    "                logs_url = (\n",
    "                    f\"https://console.aws.amazon.com/cloudwatch/home?region={region}\"\n",
    "                    f\"#logsV2:log-groups/log-group/{log_group.replace('/', '$252F')}\"\n",
    "                    f\"/log-events/{log_stream.replace('/', '$252F')}\"\n",
    "                )\n",
    "                print(f\"   {logs_url}\")\n",
    "                \n",
    "            elif current_status == 'STOPPED':\n",
    "                print(\"\\n\u26a0\ufe0f  Compilation job was stopped.\\n\")\n",
    "                print(\"The compilation job was manually stopped before completion.\")\n",
    "            \n",
    "            # Exit monitoring loop\n",
    "            break\n",
    "        \n",
    "        # Display progress indicators based on status\n",
    "        if current_status == 'STARTING':\n",
    "            # Display asterisks for starting status\n",
    "            print('*', end='', flush=True)\n",
    "            asterisks_printed += 1\n",
    "            \n",
    "            # New line every 60 asterisks (60 minutes)\n",
    "            if asterisks_printed >= 60:\n",
    "                print()\n",
    "                asterisks_printed = 0\n",
    "                \n",
    "        elif current_status == 'INPROGRESS':\n",
    "            # Display dots for in-progress status\n",
    "            print('.', end='', flush=True)\n",
    "            dots_printed += 1\n",
    "            \n",
    "            # New line every 60 dots (60 minutes)\n",
    "            if dots_printed >= 60:\n",
    "                print()\n",
    "                dots_printed = 0\n",
    "        \n",
    "        # Wait 60 seconds before next poll\n",
    "        time.sleep(60)\n",
    "        \n",
    "    except ClientError as e:\n",
    "        error_code = e.response['Error']['Code']\n",
    "        error_msg = e.response['Error']['Message']\n",
    "        print(f\"\\n\u274c Error monitoring compilation job: {error_code}\")\n",
    "        print(f\"   Job name: {compilation_job_to_monitor}\")\n",
    "        print(f\"   Error: {error_msg}\")\n",
    "        \n",
    "        if error_code == 'ValidationException':\n",
    "            print(f\"\\n\ud83d\udd0d The compilation job may not exist or the name is incorrect.\")\n",
    "            print(f\"   Verify job name: {compilation_job_to_monitor}\")\n",
    "            print(f\"   Check SageMaker console for available compilation jobs.\")\n",
    "        \n",
    "        raise Exception(\n",
    "            f\"Failed to monitor compilation job.\\n\"\n",
    "            f\"Job name: {compilation_job_to_monitor}\\n\"\n",
    "            f\"Error: {error_code} - {error_msg}\"\n",
    "        )\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n\u26a0\ufe0f  Monitoring interrupted by user.\")\n",
    "        print(f\"   Compilation job is still running: {compilation_job_to_monitor}\")\n",
    "        print(f\"   You can resume monitoring by re-running this cell.\")\n",
    "        break\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\u274c Unexpected error: {str(e)}\")\n",
    "        print(f\"   Compilation job: {compilation_job_to_monitor}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison\n",
    "\n",
    "Compare YOLO results with Lookout for Vision baseline to validate the YOLO implementation and assess model performance differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Import Comparison Functions\n",
    "\n",
    "Import the model comparison helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comparison functions\n",
    "from yolo_comparison import (\n",
    "    load_test_images,\n",
    "    run_yolo_inference,\n",
    "    calculate_detection_metrics,\n",
    "    calculate_segmentation_metrics,\n",
    "    visualize_detections,\n",
    "    visualize_segmentation,\n",
    "    create_comparison_table\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"\u2705 Comparison functions imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Load Test Images\n",
    "\n",
    "Load a set of test images from the cookie dataset for model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test images directory\n",
    "test_images_dir = 'cookie-dataset/dataset-files/training-images'\n",
    "\n",
    "print(f\"\ud83d\udcc2 Loading test images from: {test_images_dir}\\n\")\n",
    "\n",
    "try:\n",
    "    # Load test images\n",
    "    test_images = load_test_images(test_images_dir)\n",
    "    \n",
    "    print(f\"\u2705 Loaded {len(test_images)} test images\")\n",
    "    \n",
    "    # Display sample images\n",
    "    print(\"\\n\ud83d\uddbc\ufe0f  Sample images:\")\n",
    "    for i, (filename, img) in enumerate(test_images[:5]):\n",
    "        print(f\"   {i+1}. {filename} - Shape: {img.shape}\")\n",
    "    \n",
    "    if len(test_images) > 5:\n",
    "        print(f\"   ... and {len(test_images) - 5} more images\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\u274c Error loading test images: {str(e)}\")\n",
    "    print(\"\\n\ud83d\udd0d Troubleshooting:\")\n",
    "    print(f\"   - Verify the test images directory exists: {test_images_dir}\")\n",
    "    print(f\"   - Ensure the dataset acquisition step completed successfully\")\n",
    "    raise\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"\u274c Error: {str(e)}\")\n",
    "    print(\"\\n\ud83d\udd0d Troubleshooting:\")\n",
    "    print(f\"   - Verify the directory contains valid image files\")\n",
    "    print(f\"   - Supported formats: .jpg, .jpeg, .png, .bmp\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Run YOLO Inference\n",
    "\n",
    "Perform inference using the trained YOLO model (detection or segmentation).\n",
    "\n",
    "**Note:** You'll need to download the compiled model from S3 first, or use the trained model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model path\n",
    "# Option 1: Use the trained model from training (before compilation)\n",
    "# model_path = 'model-preparation/extracted/best.pt'\n",
    "\n",
    "# Option 2: Specify a local model path\n",
    "model_path = input(\"Enter path to YOLO model file (.pt): \")\n",
    "\n",
    "# Verify model exists\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"\u274c Model file not found: {model_path}\")\n",
    "    print(\"\\n\ud83d\udd0d Options:\")\n",
    "    print(\"   1. Download the trained model from S3\")\n",
    "    print(\"   2. Use a pre-trained YOLO model\")\n",
    "    print(\"   3. Extract the model from training artifacts\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"Model file not found: {model_path}\\n\"\n",
    "        f\"Expected location: {os.path.abspath(model_path)}\"\n",
    "    )\n",
    "\n",
    "# Define task type (detect or segment)\n",
    "task_type = 'detect'  # Change to 'segment' for segmentation models\n",
    "conf_threshold = 0.25  # Confidence threshold for predictions\n",
    "\n",
    "print(f\"\\n\ud83e\udd16 Running YOLO inference...\")\n",
    "print(f\"   Model: {model_path}\")\n",
    "print(f\"   Task: {task_type}\")\n",
    "print(f\"   Confidence threshold: {conf_threshold}\")\n",
    "print(f\"   Test images: {len(test_images)}\\n\")\n",
    "\n",
    "try:\n",
    "    # Run inference\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    yolo_predictions = run_yolo_inference(\n",
    "        model_path=model_path,\n",
    "        images=test_images,\n",
    "        task=task_type,\n",
    "        conf_threshold=conf_threshold\n",
    "    )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    avg_inference_time_ms = (inference_time / len(test_images)) * 1000\n",
    "    \n",
    "    print(f\"\u2705 Inference completed successfully\")\n",
    "    print(f\"   Total time: {inference_time:.2f} seconds\")\n",
    "    print(f\"   Average time per image: {avg_inference_time_ms:.2f} ms\")\n",
    "    \n",
    "    # Count detections\n",
    "    total_detections = sum(len(pred['boxes']) for pred in yolo_predictions.values())\n",
    "    images_with_detections = sum(1 for pred in yolo_predictions.values() if len(pred['boxes']) > 0)\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Detection Statistics:\")\n",
    "    print(f\"   Total detections: {total_detections}\")\n",
    "    print(f\"   Images with detections: {images_with_detections}/{len(test_images)}\")\n",
    "    print(f\"   Images without detections: {len(test_images) - images_with_detections}/{len(test_images)}\")\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\n\ud83d\udd0d Sample Predictions:\")\n",
    "    for i, (filename, pred) in enumerate(list(yolo_predictions.items())[:3]):\n",
    "        num_boxes = len(pred['boxes'])\n",
    "        if num_boxes > 0:\n",
    "            avg_conf = np.mean(pred['confidences'])\n",
    "            print(f\"   {filename}: {num_boxes} detection(s), avg confidence: {avg_conf:.3f}\")\n",
    "        else:\n",
    "            print(f\"   {filename}: No detections\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\u274c Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Error: {str(e)}\")\n",
    "    print(\"\\n\ud83d\udd0d Install Ultralytics YOLO:\")\n",
    "    print(\"   pip install ultralytics\")\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Inference failed: {str(e)}\")\n",
    "    print(\"\\n\ud83d\udd0d Troubleshooting:\")\n",
    "    print(\"   - Verify the model file is a valid YOLO model (.pt)\")\n",
    "    print(\"   - Check that the task type matches the model (detect vs segment)\")\n",
    "    print(\"   - Ensure sufficient memory is available\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Load Lookout for Vision Reference Results (Optional)\n",
    "\n",
    "Load reference results from Lookout for Vision if available for comparison.\n",
    "\n",
    "**Note:** This step is optional. Skip if you don't have LFV reference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LFV reference results (if available)\n",
    "# This is a placeholder - you would need to implement loading LFV results\n",
    "# based on your specific LFV output format\n",
    "\n",
    "lfv_results_available = False\n",
    "lfv_metrics = None\n",
    "\n",
    "print(\"\u2139\ufe0f  Lookout for Vision reference results not available\")\n",
    "print(\"   Comparison will show YOLO metrics only\")\n",
    "print(\"\\n\ud83d\udcdd To add LFV comparison:\")\n",
    "print(\"   1. Export LFV inference results\")\n",
    "print(\"   2. Load results in the same format as YOLO predictions\")\n",
    "print(\"   3. Calculate metrics using calculate_detection_metrics()\")\n",
    "\n",
    "# Example structure for LFV metrics (if you have them):\n",
    "# lfv_metrics = {\n",
    "#     'precision': 0.89,\n",
    "#     'recall': 0.85,\n",
    "#     'f1_score': 0.87,\n",
    "#     'avg_inference_time_ms': 120.5\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Calculate and Display Metrics\n",
    "\n",
    "Calculate detection metrics for YOLO predictions and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, we'll create ground truth from the manifest\n",
    "# In a real scenario, you would have separate test set annotations\n",
    "\n",
    "print(\"\ud83d\udcca Calculating metrics...\\n\")\n",
    "\n",
    "# Load ground truth annotations from manifest\n",
    "# Note: In production, you should use a separate test set\n",
    "from yolo_format_converter import read_manifest\n",
    "\n",
    "manifest_path = 'cookie-dataset/dataset-files/manifests/output.manifest'\n",
    "manifest_records = read_manifest(manifest_path)\n",
    "\n",
    "# Create ground truth dictionary\n",
    "# For simplicity, we'll use the same format as predictions\n",
    "# In a real scenario, you would have actual bounding box annotations\n",
    "ground_truth = {}\n",
    "\n",
    "for record in manifest_records:\n",
    "    filename = os.path.basename(record['source-ref'])\n",
    "    class_id = record['anomaly-label']\n",
    "    \n",
    "    # For this example, we'll create a simple ground truth\n",
    "    # In production, you would load actual bounding box annotations\n",
    "    if class_id == 1:  # Anomaly\n",
    "        # Placeholder: In reality, load actual bounding boxes\n",
    "        ground_truth[filename] = {\n",
    "            'boxes': [[100, 100, 200, 200]],  # Placeholder box\n",
    "            'classes': [1],\n",
    "            'confidences': [1.0]\n",
    "        }\n",
    "    else:  # Normal\n",
    "        ground_truth[filename] = {\n",
    "            'boxes': [],\n",
    "            'classes': [],\n",
    "            'confidences': []\n",
    "        }\n",
    "\n",
    "# Calculate detection metrics\n",
    "yolo_metrics = calculate_detection_metrics(\n",
    "    predictions=yolo_predictions,\n",
    "    ground_truth=ground_truth,\n",
    "    iou_threshold=0.5\n",
    ")\n",
    "\n",
    "# Add inference time to metrics\n",
    "yolo_metrics['avg_inference_time_ms'] = avg_inference_time_ms\n",
    "\n",
    "# Display metrics\n",
    "print(\"\u2705 Metrics calculated successfully\\n\")\n",
    "print(\"\ud83d\udcca YOLO Model Performance:\")\n",
    "print(f\"   Precision: {yolo_metrics['precision']:.4f}\")\n",
    "print(f\"   Recall: {yolo_metrics['recall']:.4f}\")\n",
    "print(f\"   F1 Score: {yolo_metrics['f1_score']:.4f}\")\n",
    "print(f\"   True Positives: {yolo_metrics['true_positives']}\")\n",
    "print(f\"   False Positives: {yolo_metrics['false_positives']}\")\n",
    "print(f\"   False Negatives: {yolo_metrics['false_negatives']}\")\n",
    "print(f\"   Avg Inference Time: {yolo_metrics['avg_inference_time_ms']:.2f} ms\")\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f  Note: Ground truth is simplified for this example.\")\n",
    "print(\"   For accurate metrics, use a separate test set with proper annotations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Create Side-by-Side Visualizations\n",
    "\n",
    "Visualize YOLO detections on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize detections on sample images\n",
    "print(\"\ud83d\uddbc\ufe0f  Creating visualizations...\\n\")\n",
    "\n",
    "# Select images with detections for visualization\n",
    "images_to_visualize = []\n",
    "for filename, pred in yolo_predictions.items():\n",
    "    if len(pred['boxes']) > 0:\n",
    "        # Find the corresponding image\n",
    "        for img_filename, img in test_images:\n",
    "            if img_filename == filename:\n",
    "                images_to_visualize.append((filename, img, pred))\n",
    "                break\n",
    "        \n",
    "        # Limit to 3 images for visualization\n",
    "        if len(images_to_visualize) >= 3:\n",
    "            break\n",
    "\n",
    "if not images_to_visualize:\n",
    "    print(\"\u26a0\ufe0f  No detections found in test images\")\n",
    "    print(\"   Try:\")\n",
    "    print(\"   - Lowering the confidence threshold\")\n",
    "    print(\"   - Using a different model\")\n",
    "    print(\"   - Checking if test images contain defects\")\n",
    "else:\n",
    "    print(f\"\u2705 Visualizing {len(images_to_visualize)} images with detections\\n\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    for filename, img, pred in images_to_visualize:\n",
    "        fig = visualize_detections(\n",
    "            image=img,\n",
    "            boxes=pred['boxes'],\n",
    "            classes=pred['classes'],\n",
    "            confidences=pred['confidences'],\n",
    "            class_names=['normal', 'anomaly'],\n",
    "            title=f\"YOLO Detections - {filename}\"\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"\\n\u2705 Visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Display Comparison Table\n",
    "\n",
    "Create and display a comparison table showing YOLO metrics (and LFV metrics if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "print(\"\ud83d\udcca Model Comparison Table\\n\")\n",
    "\n",
    "comparison_table = create_comparison_table(\n",
    "    yolo_metrics=yolo_metrics,\n",
    "    lfv_metrics=lfv_metrics  # Will be None if not available\n",
    ")\n",
    "\n",
    "print(comparison_table)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\ud83d\udcdd Summary:\")\n",
    "print(f\"   YOLO Model: {hyperparameters['model-size']}\")\n",
    "print(f\"   Task: {hyperparameters['task']}\")\n",
    "print(f\"   Test Images: {len(test_images)}\")\n",
    "print(f\"   Detections: {sum(len(pred['boxes']) for pred in yolo_predictions.values())}\")\n",
    "\n",
    "if yolo_metrics['f1_score'] >= 0.8:\n",
    "    print(\"\\n\u2705 Model performance is good (F1 \u2265 0.8)\")\n",
    "elif yolo_metrics['f1_score'] >= 0.6:\n",
    "    print(\"\\n\u26a0\ufe0f  Model performance is moderate (0.6 \u2264 F1 < 0.8)\")\n",
    "    print(\"   Consider:\")\n",
    "    print(\"   - Training for more epochs\")\n",
    "    print(\"   - Using a larger model (yolov8s or yolov8m)\")\n",
    "    print(\"   - Adjusting confidence threshold\")\n",
    "else:\n",
    "    print(\"\\n\u274c Model performance needs improvement (F1 < 0.6)\")\n",
    "    print(\"   Recommendations:\")\n",
    "    print(\"   - Increase training epochs\")\n",
    "    print(\"   - Use a larger model\")\n",
    "    print(\"   - Review training data quality\")\n",
    "    print(\"   - Adjust hyperparameters\")\n",
    "\n",
    "print(\"\\n\ud83c\udf89 Model comparison complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}